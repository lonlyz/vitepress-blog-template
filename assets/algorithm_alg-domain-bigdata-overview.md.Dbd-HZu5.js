import{_ as a,c as l,ai as t,o as r}from"./chunks/framework.BrYByd3F.js";const m=JSON.parse('{"title":"大数据处理 - Overview","description":"","frontmatter":{},"headers":[],"relativePath":"algorithm/alg-domain-bigdata-overview.md","filePath":"algorithm/alg-domain-bigdata-overview.md","lastUpdated":1737706346000}'),i={name:"algorithm/alg-domain-bigdata-overview.md"};function o(d,e,h,c,n,s){return r(),l("div",null,e[0]||(e[0]=[t('<h1 id="大数据处理-overview" tabindex="-1">大数据处理 - Overview <a class="header-anchor" href="#大数据处理-overview" aria-label="Permalink to &quot;大数据处理 - Overview&quot;">​</a></h1><blockquote><p>本文主要介绍大数据处理的一些思路。@pdai</p></blockquote><h2 id="何谓海量数据处理" tabindex="-1">何谓海量数据处理? <a class="header-anchor" href="#何谓海量数据处理" aria-label="Permalink to &quot;何谓海量数据处理?&quot;">​</a></h2><p>所谓海量数据处理，无非就是基于海量数据上的存储、处理、操作。何谓海量，就是数据量太大，所以导致要么是无法在较短时间内迅速解决，要么是数据太大，导致无法一次性装入内存。</p><p>那解决办法呢?</p><ul><li><code>针对时间</code>: 我们可以采用巧妙的算法搭配合适的数据结构，如Bloom filter/Hash/bit-map/堆/数据库或倒排索引/trie树；</li><li><code>针对空间</code>: 无非就一个办法: 大而化小，分而治之(hash映射);</li><li><code>集群|分布式</code>: 通俗点来讲，单机就是处理装载数据的机器有限(只要考虑cpu，内存，硬盘的数据交互); 而集群适合分布式处理，并行计算(更多考虑节点和节点间的数据交互)。</li></ul><h2 id="具体思路" tabindex="-1">具体思路 <a class="header-anchor" href="#具体思路" aria-label="Permalink to &quot;具体思路&quot;">​</a></h2><ul><li><a href="https://pdai.tech/md/algorithm/alg-domain-bigdata-devide-and-hash.html" target="_blank" rel="noreferrer">大数据处理 - 分治/hash/排序</a><ul><li>就是先映射，而后统计，最后排序:</li><li><code>分而治之/hash映射</code>: 针对数据太大，内存受限，只能是: 把大文件化成(取模映射)小文件，即16字方针: 大而化小，各个击破，缩小规模，逐个解决</li><li><code>hash_map统计</code>: 当大文件转化了小文件，那么我们便可以采用常规的hash_map(ip，value)来进行频率统计。</li><li><code>堆/快速排序</code>: 统计完了之后，便进行排序(可采取堆排序)，得到次数最多的IP。</li></ul></li><li><a href="https://pdai.tech/md/algorithm/alg-domain-bigdata-bloom-filter.html" target="_blank" rel="noreferrer">大数据处理 - Bitmap &amp; Bloom Filter</a><ul><li>布隆过滤器有着广泛的应用，对于大量数据的“存不存在”的问题在空间上有明显优势，但是在判断存不存在是有一定的错误率(false positive)，也就是说，有可能把不属于这个集合的元素误认为属于这个集合(False Positive)，但不会把属于这个集合的元素误认为不属于这个集合(False Negative)</li></ul></li><li><a href="https://pdai.tech/md/algorithm/alg-domain-bigdata-bucket.html" target="_blank" rel="noreferrer">大数据处理 - 双层桶划分</a><ul><li>其实本质上还是分而治之的思想，重在“分”的技巧上！<code>适用范围</code>: 第k大，中位数，不重复或重复的数字；<code>基本原理及要点</code>: 因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。</li></ul></li><li><a href="https://pdai.tech/md/algorithm/alg-domain-bigdata-db-index.html" target="_blank" rel="noreferrer">大数据处理 - Trie树/数据库/倒排索引</a><ul><li><code>适用范围</code>: 数据量大，重复多，但是数据种类小可以放入内存；<code>基本原理及要点</code>: 实现方式，节点孩子的表示方式；<code>扩展</code>: 压缩实现</li></ul></li><li><a href="https://pdai.tech/md/algorithm/alg-domain-bigdata-outsort.html" target="_blank" rel="noreferrer">大数据处理 - 外排序</a><ul><li><code>适用范围</code>: 大数据的排序，去重；<code>基本原理及要点</code>: 外排序的归并方法，置换选择败者树原理，最优归并树</li></ul></li><li><a href="https://pdai.tech/md/algorithm/alg-domain-bigdata-map-reduce.html" target="_blank" rel="noreferrer">大数据处理 - Map &amp; Reduce</a><ul><li>MapReduce是一种计算模型，简单的说就是将大批量的工作(数据)分解(MAP)执行，然后再将结果合并成最终结果(REDUCE)。这样做的好处是可以在任务被分解后，可以通过大量机器进行并行计算，减少整个操作的时间。但如果你要我再通俗点介绍，那么，说白了，Mapreduce的原理就是一个归并排序</li></ul></li></ul><h2 id="参考文章" tabindex="-1">参考文章 <a class="header-anchor" href="#参考文章" aria-label="Permalink to &quot;参考文章&quot;">​</a></h2><ul><li><a href="https://blog.csdn.net/v%5C_july%5C_v/article/category/1106578" target="_blank" rel="noreferrer">https://blog.csdn.net/v\\_july\\_v/article/category/1106578</a></li><li><a href="https://blog.csdn.net/v%5C_JULY%5C_v/article/details/6279498" target="_blank" rel="noreferrer">https://blog.csdn.net/v\\_JULY\\_v/article/details/6279498</a></li><li><a href="https://blog.csdn.net/v%5C_JULY%5C_v/article/details/7382693" target="_blank" rel="noreferrer">https://blog.csdn.net/v\\_JULY\\_v/article/details/7382693</a></li><li><a href="https://blog.csdn.net/meng984611383/article/details/80060096" target="_blank" rel="noreferrer">https://blog.csdn.net/meng984611383/article/details/80060096</a></li></ul><p>本文转自 <a href="https://pdai.tech" target="_blank" rel="noreferrer">https://pdai.tech</a>，如有侵权，请联系删除。</p>',11)]))}const g=a(i,[["render",o]]);export{m as __pageData,g as default};
