import{_ as n,c as e,ai as s,o as p}from"./chunks/framework.BrYByd3F.js";const t="/vitepress-blog-template/images/arch/gateway/gateway-vivo-1.jpeg",l="/vitepress-blog-template/images/arch/gateway/gateway-vivo-2.png",i="/vitepress-blog-template/images/arch/gateway/gateway-vivo-3.png",r="/vitepress-blog-template/images/arch/gateway/gateway-vivo-4.png",o="/vitepress-blog-template/images/arch/gateway/gateway-vivo-5.png",c="/vitepress-blog-template/images/arch/gateway/gateway-vivo-6.png",u="/vitepress-blog-template/images/arch/gateway/gateway-vivo-7.png",h="/vitepress-blog-template/images/arch/gateway/gateway-vivo-8.png",d="/vitepress-blog-template/images/arch/gateway/gateway-vivo-9.png",g="/vitepress-blog-template/images/arch/gateway/gateway-vivo-10.png",b="/vitepress-blog-template/images/arch/gateway/gateway-vivo-11.png",v="/vitepress-blog-template/images/arch/gateway/gateway-vivo-12.png",H=JSON.parse('{"title":"vivo: 微服务 API 网关架构实践","description":"","frontmatter":{},"headers":[],"relativePath":"arch/gateway/arch-gateway-vivo.md","filePath":"arch/gateway/arch-gateway-vivo.md","lastUpdated":1737706346000}'),m={name:"arch/gateway/arch-gateway-vivo.md"};function y(q,a,f,_,P,k){return p(),e("div",null,a[0]||(a[0]=[s('<h1 id="vivo-微服务-api-网关架构实践" tabindex="-1">vivo: 微服务 API 网关架构实践 <a class="header-anchor" href="#vivo-微服务-api-网关架构实践" aria-label="Permalink to &quot;vivo: 微服务 API 网关架构实践&quot;">​</a></h1><blockquote><p>vivo微服务网关的选型和设计：在进行技术选型的时候，主要考虑功能丰富度、性能、稳定性。在反复对比之后，决定选择基于Netty框架进行网关开发；但是考虑到时间的紧迫性，最终选择为针对 Zuul2 进行定制化开发，在 Zuul2 的代码骨架之上去完善网关的整个体系。</p></blockquote><h2 id="一、背景介绍" tabindex="-1">一、背景介绍 <a class="header-anchor" href="#一、背景介绍" aria-label="Permalink to &quot;一、背景介绍&quot;">​</a></h2><p>网关作为微服务生态中的重要一环，由于历史原因，中间件团队没有统一的微服务API网关，为此准备技术预研打造一个功能齐全、可用性高的业务网关。</p><h2 id="二、技术选型" tabindex="-1">二、技术选型 <a class="header-anchor" href="#二、技术选型" aria-label="Permalink to &quot;二、技术选型&quot;">​</a></h2><p>常见的开源网关按照语言分类有如下几类：</p><ul><li>Nginx+Lua：OpenResty、Kong 等；</li><li>Java：Zuul1/Zuul2、Spring Cloud Gateway、gravitee-gateway、Dromara Soul 等；</li><li>Go：janus、GoKu API Gateway 等；</li><li>Node.js：Express Gateway、MicroGateway 等。</li></ul><p>由于团队内成员基本上为Java技术栈，因此并不打算深入研究非Java语言的网关。<strong>接下来我们主要调研了Zuul1、Zuul2、Spring Cloud Gateway、Dromara Soul</strong>。</p><p>业界主流的网关基本上可以分为下面三种：</p><ul><li><strong>Servlet + 线程池</strong></li><li><strong>NIO(Tomcat / Jetty) + Servlet 3.0 异步</strong></li><li><strong>NettyServer + NettyClient</strong></li></ul><p>在进行技术选型的时候，主要考虑功能丰富度、性能、稳定性。在反复对比之后，决定选择基于Netty框架进行网关开发；但是考虑到时间的紧迫性，最终选择为针对 Zuul2 进行定制化开发，在 Zuul2 的代码骨架之上去完善网关的整个体系。</p><h2 id="三、zuul2-介绍" tabindex="-1">三、Zuul2 介绍 <a class="header-anchor" href="#三、zuul2-介绍" aria-label="Permalink to &quot;三、Zuul2 介绍&quot;">​</a></h2><p>接下来我们简要介绍一下 Zuul2 关键知识点。</p><p>Zuul2 的架构图：</p><p><img src="'+t+`" alt="error.图片加载失败"></p><p>为了解释上面这张图，接下来会分别介绍几个点</p><ul><li>如何解析 HTTP 协议</li><li>Zuul2 的数据流转</li><li>两个责任链：Netty ChannelPipeline责任链 + Filter责任链</li></ul><h3 id="_3-1-如何解析-http-协议" tabindex="-1">3.1 如何解析 HTTP 协议 <a class="header-anchor" href="#_3-1-如何解析-http-协议" aria-label="Permalink to &quot;3.1 如何解析 HTTP 协议&quot;">​</a></h3><blockquote><p>学习Zuul2需要一定的铺垫知识，比如：Google Guice、RxJava、Netflix archaius等，但是更关键的应该是：如何解析HTTP协议，会影响到后续Filter责任链的原理解析，为此先分析这个关键点。</p></blockquote><p>首先我们介绍<a href="https://github.com/Netflix/zuul/wiki/Filters" target="_blank" rel="noreferrer">官方文档在新窗口打开</a>中的一段话：</p><p>官方文档</p><p>By default Zuul doesn&#39;t buffer body content, meaning it streams the received headers to the origin before the body has been received.</p><p>默认情况下Zuul2并不会缓存请求体，也就意味着它可能会先发送接收到的请求Headers到后端服务，之后接收到请求体再继续发送到后端服务，发送请求体的时候，也不是组装为一个完整数据之后才发，而是接收到一部分，就转发一部分。</p><p>This streaming behavior is very efficient and desirable, as long as your filter logic depends on header data.</p><p>这个流式行为是高效的，只要Filter过滤的时候只依赖Headers的数据进行逻辑处理，而不需要解析RequestBody。</p><p>上面这段话映射到Netty Handler中，则意味着Zuul2并没有使用HttpObjectAggregator。</p><p>我们先看一下常规的Netty Server处理HTTP协议的样例：</p><p>NettyServer样例</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>@Slf4j</span></span>
<span class="line"><span>public class ConfigServerBootstrap {</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>    public static final int WORKER_THREAD_COUNT = Runtime.getRuntime().availableProcessors();</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>    public void start(){</span></span>
<span class="line"><span>        int port = 8080;</span></span>
<span class="line"><span>        EventLoopGroup bossGroup = new NioEventLoopGroup(1);</span></span>
<span class="line"><span>        EventLoopGroup workerGroup = new NioEventLoopGroup(WORKER_THREAD_COUNT);</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>        final BizServerHandler bizServerHandler = new BizServerHandler();</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>        try {</span></span>
<span class="line"><span>            ServerBootstrap serverBootstrap = new ServerBootstrap();</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>            serverBootstrap.group(bossGroup, workerGroup)</span></span>
<span class="line"><span>                    .channel(NioServerSocketChannel.class)</span></span>
<span class="line"><span>                    .childHandler(new ChannelInitializer&lt;Channel&gt;() {</span></span>
<span class="line"><span>                        @Override</span></span>
<span class="line"><span>                        protected void initChannel(Channel ch) throws Exception {</span></span>
<span class="line"><span>                            ChannelPipeline pipeline = ch.pipeline();</span></span>
<span class="line"><span>                            pipeline.addLast(new IdleStateHandler(10, 10, 0));</span></span>
<span class="line"><span>                            pipeline.addLast(new HttpServerCodec());</span></span>
<span class="line"><span>                            pipeline.addLast(new HttpObjectAggregator(500 * 1024 * 1024));</span></span>
<span class="line"><span>                            pipeline.addLast(bizServerHandler);</span></span>
<span class="line"><span>                        }</span></span>
<span class="line"><span>                    });</span></span>
<span class="line"><span>            log.info(&quot;start netty server, port:{}&quot;, port);</span></span>
<span class="line"><span>            serverBootstrap.bind(port).sync();</span></span>
<span class="line"><span>        } catch (InterruptedException e) {</span></span>
<span class="line"><span>            bossGroup.shutdownGracefully();</span></span>
<span class="line"><span>            workerGroup.shutdownGracefully();</span></span>
<span class="line"><span>            log.error(String.format(&quot;start netty server error, port:%s&quot;, port), e);</span></span>
<span class="line"><span>        }</span></span>
<span class="line"><span>    }</span></span>
<span class="line"><span>}</span></span></code></pre></div><p>这个例子中的两个关键类为：HttpServerCodec、HttpObjectAggregator。</p><p>HttpServerCodec是HttpRequestDecoder、HttpResponseEncoder的组合器。</p><ul><li><strong>HttpRequestDecoder职责</strong>：将输入的ByteBuf解析成HttpRequest、HttpContent对象。</li><li><strong>HttpResponseEncoder职责</strong>：将HttpResponse、HttpContent对象转换为ByteBuf，进行网络二进制流的输出。</li></ul><p>HttpObjectAggregator的作用：组装HttpMessage、HttpContent为一个完整的FullHttpRequest或者FullHttpResponse。</p><p>当你不想关心chunked分块传输的时候，使用HttpObjectAggregator是非常有用的。</p><p>HTTP协议通常使用Content-Length来标识body的长度，在服务器端，需要先申请对应长度的buffer，然后再赋值。如果需要一边生产数据一边发送数据，就需要使用&quot;Transfer-Encoding: chunked&quot; 来代替Content-Length，也就是对数据进行分块传输。</p><p>接下来我们看一下Zuul2为了解析HTTP协议做了哪些处理。</p><p>Zuul的源码：<a href="https://github.com/Netflix/zuul%EF%BC%8C%E5%9F%BA%E4%BA%8Ev2.1.5%E3%80%82" target="_blank" rel="noreferrer">https://github.com/Netflix/zuul，基于v2.1.5。</a></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// com.netflix.zuul.netty.server.BaseZuulChannelInitializer#addHttp1Handlers</span></span>
<span class="line"><span>protected void addHttp1Handlers(ChannelPipeline pipeline) {</span></span>
<span class="line"><span>    pipeline.addLast(HTTP_CODEC_HANDLER_NAME, createHttpServerCodec());</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>    pipeline.addLast(new Http1ConnectionCloseHandler(connCloseDelay));</span></span>
<span class="line"><span>    pipeline.addLast(&quot;conn_expiry_handler&quot;,</span></span>
<span class="line"><span>            new Http1ConnectionExpiryHandler(maxRequestsPerConnection, maxRequestsPerConnectionInBrownout, connectionExpiry));</span></span>
<span class="line"><span>}</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// com.netflix.zuul.netty.server.BaseZuulChannelInitializer#createHttpServerCodec</span></span>
<span class="line"><span>protected HttpServerCodec createHttpServerCodec() {</span></span>
<span class="line"><span>    return new HttpServerCodec(</span></span>
<span class="line"><span>            MAX_INITIAL_LINE_LENGTH.get(),</span></span>
<span class="line"><span>            MAX_HEADER_SIZE.get(),</span></span>
<span class="line"><span>            MAX_CHUNK_SIZE.get(),</span></span>
<span class="line"><span>            false</span></span>
<span class="line"><span>    );</span></span>
<span class="line"><span>}</span></span></code></pre></div><p>通过对比上面的样例发现，Zuul2并没有添加HttpObjectAggregator，也就是需要自行去处理chunked分块传输问题、自行组装请求体数据。</p><p>为了解决上面说的chunked分块传输问题，Zuul2通过判断是否LastHttpContent，来判断是否接收完成。</p><h3 id="_3-2-zuul2-数据流转" tabindex="-1">3.2 Zuul2 数据流转 <a class="header-anchor" href="#_3-2-zuul2-数据流转" aria-label="Permalink to &quot;3.2 Zuul2 数据流转&quot;">​</a></h3><p><img src="`+l+'" alt="error.图片加载失败"></p><p>如上图所示，Netty自带的HttpServerCodec会将网络二进制流转换为Netty的HttpRequest对象，再通过ClientRequestReceiver编解码器将HttpRequest转换为Zuul的请求对象HttpRequestMessageImpl；</p><p>请求体RequestBody在Netty自带的HttpServerCodec中被映射为HttpContent对象，ClientRequestReceiver编解码器依次接收HttpContent对象。</p><p>完成了上述数据的转换之后，就流转到了最重要的编解码ZuulFilterChainHandler，里面会执行Filter链，也会发起网络请求到真正的后端服务，这一切都是在ZuulFilterChainHandler中完成的。</p><p>得到了后端服务的响应结果之后，也经过了Outbound Filter的过滤，接下来就是通过ClientResponseWriter把Zuul自定义的响应对象HttpResponseMessageImpl转换为Netty的HttpResponse对象，然后通过HttpServerCodec转换为ByteBuf对象，发送网络二进制流，完成响应结果的输出。</p><p>这里需要特别说明的是：由于Zuul2默认不组装一个完整的请求对象/响应对象，所以Zuul2是分别针对请求头+请求Headers、请求体进行Filter过滤拦截的，也就是说对于请求，会走两遍前置Filter链，对于响应结果，也是会走两遍后置Filter链拦截。</p><h3 id="_3-3-两个责任链" tabindex="-1">3.3 两个责任链 <a class="header-anchor" href="#_3-3-两个责任链" aria-label="Permalink to &quot;3.3 两个责任链&quot;">​</a></h3><h4 id="_3-3-1-netty-channelpipeline责任链" tabindex="-1">3.3.1 Netty ChannelPipeline责任链 <a class="header-anchor" href="#_3-3-1-netty-channelpipeline责任链" aria-label="Permalink to &quot;3.3.1 Netty ChannelPipeline责任链&quot;">​</a></h4><p>Netty的ChannelPipeline设计，通过往ChannelPipeline中动态增减Handler进行定制扩展。</p><p>接下来看一下Zuul2 Netty Server中的pipeline有哪些Handler？</p><p><img src="'+i+'" alt="error.图片加载失败"></p><p>接着继续看一下Zuul2 Netty Client的Handler有哪些？</p><p><img src="'+r+'" alt="error.图片加载失败"></p><p>本文不针对具体的Handler进行详细解释，主要是给大家一个整体的视图。</p><h4 id="_3-3-2-filter责任链" tabindex="-1">3.3.2 Filter责任链 <a class="header-anchor" href="#_3-3-2-filter责任链" aria-label="Permalink to &quot;3.3.2 Filter责任链&quot;">​</a></h4><p><img src="'+o+'" alt="error.图片加载失败"></p><p>请求发送到Netty Server中，先进行Inbound Filters的拦截处理，接着会调用Endpoint Filter，这里默认为ProxyEndPoint（里面封装了Netty Client），发送请求到真实后端服务，获取到响应结果之后，再执行Outbound Filters，最终返回响应结果。</p><p>三种类型的Filter之间是通过nextStage属性来衔接的。</p><p>Zuul2存在一个定时任务线程GroovyFilterFileManagerPoller，定期扫描特定的目录，通过比对文件的更新时间戳，来判断是否发生变化，如果有变化，则重新编译并放入到内存中。</p><p>通过定位任务实现了Filter的动态加载。</p><h2 id="四、功能介绍" tabindex="-1">四、功能介绍 <a class="header-anchor" href="#四、功能介绍" aria-label="Permalink to &quot;四、功能介绍&quot;">​</a></h2><p>上面介绍了Zuul2的部分知识点，接下来介绍网关的整体功能。</p><p><img src="'+c+'" alt="error.图片加载失败"></p><h3 id="_4-1-服务注册发现" tabindex="-1">4.1 服务注册发现 <a class="header-anchor" href="#_4-1-服务注册发现" aria-label="Permalink to &quot;4.1 服务注册发现&quot;">​</a></h3><blockquote><p>网关承担了请求转发的功能，需要一定的方法用于动态发现后端服务的机器列表。</p></blockquote><p>这里提供两种方式进行服务的注册发现：</p><p><strong>集成网关SDK</strong></p><ul><li>网关SDK会在服务启动之后，监听ContextRefreshedEvent事件，主动操作zk登记信息到zookeeper注册中心，这样网关服务、网关管理后台就可以订阅节点信息。</li><li>网关SDK添加了ShutdownHook，在服务下线的时候，会删除登记在zk的节点信息，用于通知网关服务、网关管理后台，节点已下线。</li></ul><p><strong>手工配置服务的机器节点信息</strong></p><ul><li>在网关管理后台，手工添加、删除机器节点。</li><li>在网关管理后台，手工设置节点上线、节点下线操。</li></ul><p>为了防止zookeeper故障，网关管理后台已提供HTTP接口用于注册、取消注册作为兜底措施。</p><h3 id="_4-2-动态路由" tabindex="-1">4.2 动态路由 <a class="header-anchor" href="#_4-2-动态路由" aria-label="Permalink to &quot;4.2 动态路由&quot;">​</a></h3><blockquote><p>动态路由分为：机房就近路由、灰度路由(类似于Dubbo的标签路由功能)。</p></blockquote><p><strong>机房就近路由</strong>：请求最好是不要跨机房，比如请求打到网关服务的X机房，那么也应该是将请求转发给X机房的后端服务节点，如果后端服务不存在X机房的节点，则请求到其他机房的节点。</p><p><strong>灰度路由</strong>：类似于Dubbo的标签路由功能，如果希望对后端服务节点进行分组隔离，则需要给后端服务一个标签名，建立&quot;标签名→节点列表&quot;的映射关系，请求方携带这个标签名，请求到相应的后端服务节点。</p><p>网关管理后台支持动态配置路由信息，动态开启/关闭路由功能。</p><h3 id="_4-3-负载均衡" tabindex="-1">4.3 负载均衡 <a class="header-anchor" href="#_4-3-负载均衡" aria-label="Permalink to &quot;4.3 负载均衡&quot;">​</a></h3><p>当前支持的负载均衡策略：加权随机算法、加权轮询算法、一致性哈希算法。</p><p>可以通过网关管理后台动态调整负载均衡策略，支持API接口级别、应用级别的配置。</p><p>负载均衡机制并未采用Netflix Ribbon，而是仿造Dubbo负载均衡的算法实现的。</p><h3 id="_4-4-动态配置" tabindex="-1">4.4 动态配置 <a class="header-anchor" href="#_4-4-动态配置" aria-label="Permalink to &quot;4.4 动态配置&quot;">​</a></h3><p>API网关支持一套自洽的动态配置功能，在不依赖第三方配置中心的条件下，仍然支持实时调整配置项，并且配置项分为全局配置、应用级别治理配置、API接口级别治理配置。</p><p>在自洽的动态配置功能之外，网关服务也与公司级别的配置中心进行打通，支持公司级配置中心配置相应的配置项。</p><h3 id="_4-5-api管理" tabindex="-1">4.5 API管理 <a class="header-anchor" href="#_4-5-api管理" aria-label="Permalink to &quot;4.5 API管理&quot;">​</a></h3><p>API管理支持网关SDK自动扫描上报，也支持在管理后台手工配置。</p><h3 id="_4-6-协议转换" tabindex="-1">4.6 协议转换 <a class="header-anchor" href="#_4-6-协议转换" aria-label="Permalink to &quot;4.6 协议转换&quot;">​</a></h3><p>后端的服务有很多是基于Dubbo框架的，网关服务支持HTTP→HTTP的请求转发，也支持HTTP→Dubbo的协议转换。</p><p>同时C++技术栈，采用了tars框架，网关服务也支持HTTP → tras协议转换。</p><h3 id="_4-7-安全机制" tabindex="-1">4.7 安全机制 <a class="header-anchor" href="#_4-7-安全机制" aria-label="Permalink to &quot;4.7 安全机制&quot;">​</a></h3><p>API网关提供了IP黑白名单、OAuth认证授权、appKey&amp;appSecret验签、矛盾加解密、vivo登录态校验的功能。</p><h3 id="_4-8-监控-告警" tabindex="-1">4.8 监控/告警 <a class="header-anchor" href="#_4-8-监控-告警" aria-label="Permalink to &quot;4.8 监控/告警&quot;">​</a></h3><p>API网关通过对接通用监控上报请求访问信息，对API接口的QPS、请求响应吗、请求响应时间等进行监控与告警；</p><p>通过对接基础监控，对网关服务自身节点进行CPU、IO、内存、网络连接等数据进行监控。</p><h3 id="_4-9-限流-熔断" tabindex="-1">4.9 限流/熔断 <a class="header-anchor" href="#_4-9-限流-熔断" aria-label="Permalink to &quot;4.9 限流/熔断&quot;">​</a></h3><p>API网关与限流熔断系统进行打通，可以在限流熔断系统进行API接口级别的配置，比如熔断配置、限流配置，而无需业务系统再次对接限流熔断组件。</p><p>限流熔断系统提供了对Netflix Hystrix、Alibaba Sentinel组件的封装。</p><h3 id="_4-10-无损发布" tabindex="-1">4.10 无损发布 <a class="header-anchor" href="#_4-10-无损发布" aria-label="Permalink to &quot;4.10 无损发布&quot;">​</a></h3><p>业务系统的无损发布，这里分为两种场景介绍：</p><ul><li><strong>集成了网关SDK</strong>：网关SDK添加了ShutdownHook，会主动从zookeeper删除登记的节点信息，从而避免请求打到即将下线的节点。</li><li><strong>未集成网关SDK</strong>：如果什么都不做，则只能依赖网关服务的心跳检测功能，会有15s的流量损失。庆幸的是管理后台提供了流量摘除、流量恢复的操作按钮，支持动态的上线、下线机器节点。</li></ul><p>网关集群的无损发布：我们考虑了后端服务的无损发布，但是也需要考虑网关节点自身的无损发布，这里我们不再重复造轮子，直接使用的是CICD系统的HTTP无损发布功能（Nginx动态摘除/上线节点）。</p><h3 id="_4-11-网关集群分组隔离" tabindex="-1">4.11 网关集群分组隔离 <a class="header-anchor" href="#_4-11-网关集群分组隔离" aria-label="Permalink to &quot;4.11 网关集群分组隔离&quot;">​</a></h3><p>网关集群的分组隔离指的是业务与业务之间的请求应该是隔离的，不应该被部分业务请求打垮了网关服务，从而导致了别的业务请求无法处理。</p><p>这里我们会对接入网关的业务进行分组归类，不同的业务使用不同的分组，不同的网关分组，会部署独立的网关集群，从而隔离了风险，不用再担心业务之间的互相影响。</p><h2 id="五、系统架构" tabindex="-1">五、系统架构 <a class="header-anchor" href="#五、系统架构" aria-label="Permalink to &quot;五、系统架构&quot;">​</a></h2><h3 id="_5-1-模块交互图" tabindex="-1">5.1 模块交互图 <a class="header-anchor" href="#_5-1-模块交互图" aria-label="Permalink to &quot;5.1 模块交互图&quot;">​</a></h3><p><img src="'+u+'" alt="error.图片加载失败"></p><h3 id="_5-2-网关管理后台" tabindex="-1">5.2 网关管理后台 <a class="header-anchor" href="#_5-2-网关管理后台" aria-label="Permalink to &quot;5.2 网关管理后台&quot;">​</a></h3><p>模块划分</p><p><img src="'+h+'" alt="error.图片加载失败"></p><h3 id="_5-3-通信机制" tabindex="-1">5.3 通信机制 <a class="header-anchor" href="#_5-3-通信机制" aria-label="Permalink to &quot;5.3 通信机制&quot;">​</a></h3><blockquote><p>由于需要动态的下发配置，比如全局开关、应用级别的治理配置、接口级别的治理配置，就需要网关管理后台可以与网关服务进行通信，比如推拉模式。</p></blockquote><p><strong>两种设计方案</strong></p><ul><li>基于注册中心的订阅通知机制</li><li>基于HTTP的推模式 + 定时拉取</li></ul><p>这里并未采用第一种方案，主要是因为以下缺点：</p><ul><li>严重依赖zk集群的稳定性</li><li>信息不私密(zk集群权限管控能力较弱、担心被误删)</li><li>无法灰度下发配置，比如只对其中的一台网关服务节点配置生效</li></ul><h4 id="_5-3-1-基于http的推模式" tabindex="-1">5.3.1 基于HTTP的推模式 <a class="header-anchor" href="#_5-3-1-基于http的推模式" aria-label="Permalink to &quot;5.3.1 基于HTTP的推模式&quot;">​</a></h4><p><img src="'+d+`" alt="error.图片加载失败"></p><p>因为Zuul2本身就自带了Netty Server，同理也可以再多启动一个Netty Server提供HTTP服务，让管理后台发送HTTP请求到网关服务，进而发送配置数据到网关服务了。</p><p>所以图上的蓝色标记Netty Server用于接收客户端请求转发到后端节点，紫色标记Netty Server用于提供HTTP服务，接收配置数据。</p><h4 id="_5-3-2-全量配置拉取" tabindex="-1">5.3.2 全量配置拉取 <a class="header-anchor" href="#_5-3-2-全量配置拉取" aria-label="Permalink to &quot;5.3.2 全量配置拉取&quot;">​</a></h4><p>网关服务在启动之初，需要发送HTTP请求到管理后台拉取全部的配置数据，并且也需要拉取归属当前节点的灰度配置(只对这个节点生效的试验性配置)。</p><h4 id="_5-3-3-增量配置定时拉取" tabindex="-1">5.3.3 增量配置定时拉取 <a class="header-anchor" href="#_5-3-3-增量配置定时拉取" aria-label="Permalink to &quot;5.3.3 增量配置定时拉取&quot;">​</a></h4><blockquote><p>上面提到了&quot;基于HTTP的推模式&quot;进行配置的动态推送，也介绍了全局配置拉取，为了保险起见，网关服务还是新增了一个定时任务，用于定时拉取增量配置。</p></blockquote><p>可以理解为兜底操作，就好比配置中心支持长轮询获取数据实时变更+定时任务获取全部数据。</p><p>在拉取到增量配置之后，会比对内存中的配置数据是否一致，如果一致，则不操作直接丢弃。</p><h4 id="_5-3-4-灰度配置下发" tabindex="-1">5.3.4 灰度配置下发 <a class="header-anchor" href="#_5-3-4-灰度配置下发" aria-label="Permalink to &quot;5.3.4 灰度配置下发&quot;">​</a></h4><blockquote><p>上面也提到了&quot;灰度配置&quot;这个词，这里详细解释一下什么是灰度配置？</p></blockquote><p>比如当编辑了某个接口的限流信息，希望在某个网关节点运行一段时间，如果没有问题，则调整配置让全部的网关服务节点生效，如果有问题，则也只是其中一个网关节点的请求流量出问题。</p><p>这样可以降低出错的概率，当某个比较大的改动或者版本上线的时候，可以控制灰度部署一台机器，同时配置也只灰度到这台机器，这样风险就降低了很多。</p><p><strong>灰度配置</strong>：可以理解为只在某些网关节点生效的配置。</p><p>灰度配置下发其实也是通过&quot;5.3.1基于HTTP的推模式&quot;来进行下发的。</p><h3 id="_5-4-网关sdk" tabindex="-1">5.4 网关SDK <a class="header-anchor" href="#_5-4-网关sdk" aria-label="Permalink to &quot;5.4 网关SDK&quot;">​</a></h3><blockquote><p>网关SDK旨在完成后端服务节点的注册与下线、API接口列表数据上报，通过接入网关SDK即可减少手工操作。网关SDK通过 ZooKeeper client操作节点的注册与下线，通过发起HTTP请求进行API接口数据的上报。</p></blockquote><p>支持SpringMVC、SpringBoot的web接口自动扫描、Dubbo新老版本的Service接口扫描。</p><ul><li><strong>Dubbo 接口上报</strong>：</li></ul><ol><li><strong>旧版Dubbo</strong>：自定义BeanPostProcessor，用于提取到ServiceBean，放入线程池异步上报到网关后台。</li><li><strong>新版Dubbo</strong>：自定义ApplicationListener，用于监听ServiceBeanExportedEvent事件，提取event信息，上报到网关后台。</li></ol><ul><li><strong>HTTP 接口上报</strong>：</li></ul><p>自定义BeanPostProcessor，用于提取到Controller、RestController的RequestMapping注解，放入线程池异步上报API信息。</p><h2 id="六、改造之路" tabindex="-1">六、改造之路 <a class="header-anchor" href="#六、改造之路" aria-label="Permalink to &quot;六、改造之路&quot;">​</a></h2><h3 id="_6-1-动态配置" tabindex="-1">6.1 动态配置 <a class="header-anchor" href="#_6-1-动态配置" aria-label="Permalink to &quot;6.1 动态配置&quot;">​</a></h3><p>关联知识点：</p><p><a href="https://github.com/apache/commons-configuration" target="_blank" rel="noreferrer">https://github.com/apache/commons-configuration</a></p><p><a href="https://github.com/Netflix/archaius" target="_blank" rel="noreferrer">https://github.com/Netflix/archaius</a></p><p>Zuul2依赖的动态配置为archaius，通过扩展ConcurrentMapConfiguration添加到ConcurrentCompositeConfiguration中。</p><p>新增GatewayConfigConfiguration，用于存储全局配置、治理配置、节点信息、API数据等。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>@Singleton</span></span>
<span class="line"><span>public class GatewayConfigConfiguration extends ConcurrentMapConfiguration {</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>    public GatewayConfigConfiguration() {</span></span>
<span class="line"><span>        /**</span></span>
<span class="line"><span>         * 设置这个值为true，才可以避免archaius强行去除value的类型，导致获取报错</span></span>
<span class="line"><span>         * see com.netflix.config.ConcurrentMapConfiguration#setPropertyImpl(java.lang.String, java.lang.Object)</span></span>
<span class="line"><span>         */</span></span>
<span class="line"><span>        this.setDelimiterParsingDisabled(Boolean.TRUE);</span></span>
<span class="line"><span>    }</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>}</span></span></code></pre></div><p>通过Google Guice控制Bean的加载顺序，在较早的时机，执行</p><p>ConfigurationManager.getConfigInstance()，获取到ConcurrentCompositeConfiguration，完成GatewayConfigConfiguration的初始化，然后再插入到第一个位置。</p><p>后续只需要对GatewayConfigConfiguration进行配置的增删查改操作即可。</p><h3 id="_6-2-路由机制" tabindex="-1">6.2 路由机制 <a class="header-anchor" href="#_6-2-路由机制" aria-label="Permalink to &quot;6.2 路由机制&quot;">​</a></h3><p><img src="`+g+'" alt="error.图片加载失败"></p><p>路由机制也是仿造的Dubbo路由机制，灰度路由是仿造的Dubbo的标签路由，就近路由可以理解为同机房路由。</p><p><strong>请求处理过程</strong>：</p><p>客户端请求过来的时候，网关服务会通过path前缀提取到对应的后端服务名或者在请求Header中指定传递对应的serviceName，然后只在匹配到的后端服务中，继续API匹配操作，如果匹配到API，则筛选出对应的后端机器列表，然后进行路由、负载均衡，最终选中一台机器，将请求转发过去。</p><p>这里会有个疑问，如果不希望只在某个后端服务中进行请求路由匹配，是希望在一堆后端服务中进行匹配，需要怎么操作？</p><p>在后面的第七章节会解答这个疑问，请耐心阅读。</p><h4 id="_6-2-1-就近路由" tabindex="-1">6.2.1 就近路由 <a class="header-anchor" href="#_6-2-1-就近路由" aria-label="Permalink to &quot;6.2.1 就近路由&quot;">​</a></h4><p>当请求到网关服务，会提取网关服务自身的机房loc属性值，读取全局、应用级别的开关，如果就近路由开关打开，则筛选服务列表的时候，会过滤相同loc的后端机器，负载均衡的时候，在相同loc的机器列表中挑选一台进行请求。</p><p>如果没有相同loc的后端机器，则降级从其他loc的后端机器中进行挑选。</p><p><img src="'+b+'" alt="error.图片加载失败"></p><p>其中 loc信息就是机房信息，每个后端服务节点在SDK上报或者手工录入的时候，都会携带这个值。</p><h4 id="_6-2-2-灰度路由" tabindex="-1">6.2.2 灰度路由 <a class="header-anchor" href="#_6-2-2-灰度路由" aria-label="Permalink to &quot;6.2.2 灰度路由&quot;">​</a></h4><p>灰度路由需要用户传递Header属性值，比如gray=canary_gray。</p><p>网关管理后台配置灰度路由的时候，会建立<code>grayName -&gt; List&lt;Server&gt;</code>映射关系，当网关管理后台增量推送到网关服务之后，网关服务就可以通过grayName来提取配置下的后端机器列表，然后再进行负载均衡挑选机器。</p><p>如下图所示：</p><p><img src="'+v+`" alt="error.图片加载失败"></p><h4 id="_6-3-api映射匹配" tabindex="-1">6.3 API映射匹配 <a class="header-anchor" href="#_6-3-api映射匹配" aria-label="Permalink to &quot;6.3 API映射匹配&quot;">​</a></h4><p>网关在进行请求转发的时候，需要明确知道请求哪一个服务的哪一个API，这个过程就是API匹配。</p><p>因为不同的后端服务可能会拥有相同路径的API，所以网关要求请求传递serviceName，serviceName可以放置于请求Header或者请求参数中。</p><p>携带了serviceName之后，就可以在后端服务的API中去匹配了，有一些是相等匹配，有些是正则匹配，因为RESTFul协议，需要支持 <code>/*</code> 通配符匹配。</p><p>这里会有人疑问了，<strong>难道请求一定需要显式传递serviceName吗</strong>？</p><p>为了解决这个问题，创建了一个gateway_origin_mapping表，用于path前缀或者域名前缀 映射到 serviceName，通过在管理后台建立这个映射关系，然后推送到网关服务，即可解决显式传递serviceName的问题，会自动提取请求的path前缀、域名前缀，找到对应的serviceName。</p><p>如果不希望是在一个后端服务中进行API匹配，则需阅读后面的第七章节。</p><h3 id="_6-4-负载均衡" tabindex="-1">6.4 负载均衡 <a class="header-anchor" href="#_6-4-负载均衡" aria-label="Permalink to &quot;6.4 负载均衡&quot;">​</a></h3><p>替换 ribbon 组件，改为仿造 Dubbo 的负载均衡机制。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>public interface ILoadBalance {</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>    /**</span></span>
<span class="line"><span>     * 从服务列表中筛选一台机器进行调用</span></span>
<span class="line"><span>     * @param serverList</span></span>
<span class="line"><span>     * @param originName</span></span>
<span class="line"><span>     * @param requestMessage</span></span>
<span class="line"><span>     * @return</span></span>
<span class="line"><span>     */</span></span>
<span class="line"><span>    DynamicServer select(List&lt;DynamicServer&gt; serverList, String originName, HttpRequestMessage requestMessage);</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>}</span></span></code></pre></div><p>替换的理由：ribbon的服务列表更新只是定期更新，如果不考虑复杂的筛选过滤，是满足要求的，但是如果想要灵活的根据请求头、请求参数进行筛选，ribbon则不太适合。</p><h3 id="_6-5-心跳检测" tabindex="-1">6.5 心跳检测 <a class="header-anchor" href="#_6-5-心跳检测" aria-label="Permalink to &quot;6.5 心跳检测&quot;">​</a></h3><blockquote><p><strong>核心思路</strong>：当网络请求正常返回的时候，心跳检测是不需要，此时后端服务节点肯定是正常的，只需要定期检测未被请求的后端节点，超过一定的错误阈值，则标记为不可用，从机器列表中剔除。</p></blockquote><p><strong>第一期先实现简单版本</strong>：通过定时任务定期去异步调用心跳检测Url，如果超过失败阈值，则从从负载均衡列表中剔除。</p><p>异步请求采用httpasyncclient组件处理。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>&lt;dependency&gt;</span></span>
<span class="line"><span>    &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;</span></span>
<span class="line"><span>    &lt;artifactId&gt;httpasyncclient&lt;/artifactId&gt;</span></span>
<span class="line"><span>    &lt;version&gt;4.1.4&lt;/version&gt;</span></span>
<span class="line"><span>&lt;/dependency&gt;</span></span></code></pre></div><p>方案为：HealthCheckScheduledExecutor + HealthCheckTask + HttpAsyncClient。</p><h3 id="_6-6-日志异步化改造" tabindex="-1">6.6 日志异步化改造 <a class="header-anchor" href="#_6-6-日志异步化改造" aria-label="Permalink to &quot;6.6 日志异步化改造&quot;">​</a></h3><p>Zuul2默认采用的log4j进行日志打印，是同步阻塞操作，需要修改为异步化操作，改为使用logback的AsyncAppender。</p><p>日志打印也是影响性能的一个关键点，需要特别注意，后续会衡量是否切换为log4j2。</p><h3 id="_6-7-协议转换" tabindex="-1">6.7 协议转换 <a class="header-anchor" href="#_6-7-协议转换" aria-label="Permalink to &quot;6.7 协议转换&quot;">​</a></h3><ul><li><strong>HTTP -&gt; HTTP</strong></li></ul><p>Zuul2采用的是ProxyEndpoint用于支持HTTP -&gt; HTTP协议转发。</p><p>通过Netty Client的方式发起网络请求到真实的后端服务。</p><ul><li><strong>HTTP -&gt; Dubbo</strong></li></ul><p>采用Dubbo的泛化调用实现HTTP -&gt; Dubbo协议转发，可以采用$invokeAsync。</p><ul><li><strong>HTTP → Tars</strong></li></ul><p>基于tars-java采用类似于Dubbo的泛化调用的方式实现协议转发，基于<a href="https://github.com/TarsCloud/TarsGateway" target="_blank" rel="noreferrer">https://github.com/TarsCloud/TarsGateway</a> 改造而来的。</p><h3 id="_6-8-无损发布" tabindex="-1">6.8 无损发布 <a class="header-anchor" href="#_6-8-无损发布" aria-label="Permalink to &quot;6.8 无损发布&quot;">​</a></h3><p>网关作为请求转发，当然希望在业务后端机器部署的期间，不应该把请求转发到还未部署完成的节点。</p><p>业务后端机器节点的无损发布，这里分为两种场景介绍：</p><ul><li><strong>集成了网关SDK</strong></li></ul><p>网关SDK添加了ShutdownHook，会主动从zookeeper删除登记的节点信息，从而避免请求打到即将下线的节点。</p><ul><li><strong>未集成网关SDK</strong></li></ul><p>如果什么都不做，则只能依赖网关服务的心跳检测功能，会有15s的流量损失。庆幸的是管理后台提供了流量摘除、流量恢复的操作按钮，支持动态的上线、下线机器节点。</p><p><strong>设计方案</strong></p><p>我们给后端机器节点dynamic_forward_server表新增了一个字段online，如果online=1，则代表在线，接收流量，反之，则代表下线，不接收流量。</p><p>网关服务gateway-server新增一个路由：OnlineRouter，从后端机器列表中筛选online=1的机器，过滤掉不在线的机器，则完成了无损发布的功能。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>public interface IRouter {</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>    /**</span></span>
<span class="line"><span>     * 过滤</span></span>
<span class="line"><span>     * @param serverList</span></span>
<span class="line"><span>     * @param originName</span></span>
<span class="line"><span>     * @param requestMessage</span></span>
<span class="line"><span>     * @return</span></span>
<span class="line"><span>     */</span></span>
<span class="line"><span>    List&lt;DynamicServer&gt; route(List&lt;DynamicServer&gt; serverList, String originName, HttpRequestMessage requestMessage);</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>}</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>package com.netflix.zuul.extension.router;</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>import com.netflix.zuul.extension.loadbalance.DynamicServer;</span></span>
<span class="line"><span>import com.netflix.zuul.message.http.HttpRequestMessage;</span></span>
<span class="line"><span>import org.apache.commons.collections.CollectionUtils;</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>import java.util.ArrayList;</span></span>
<span class="line"><span>import java.util.List;</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>/**</span></span>
<span class="line"><span> * 在线机器节点_路由</span></span>
<span class="line"><span> */</span></span>
<span class="line"><span>public class OnlineRouter implements IRouter {</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>    @Override</span></span>
<span class="line"><span>    public List&lt;DynamicServer&gt; route(List&lt;DynamicServer&gt; serverList, String originName, HttpRequestMessage requestMessage) {</span></span>
<span class="line"><span>        boolean hasOfflineServer = hasOfflineMachines(serverList);</span></span>
<span class="line"><span>        if (hasOfflineServer) {</span></span>
<span class="line"><span>            // 进行过滤</span></span>
<span class="line"><span>            List&lt;DynamicServer&gt; retServerList = new ArrayList&lt;&gt;();</span></span>
<span class="line"><span>            for (DynamicServer dynamicServer : serverList) {</span></span>
<span class="line"><span>                if (dynamicServer.getOnline() == null || dynamicServer.getOnline().intValue() == 1) {</span></span>
<span class="line"><span>                    retServerList.add(dynamicServer);</span></span>
<span class="line"><span>                }</span></span>
<span class="line"><span>            }</span></span>
<span class="line"><span>            return retServerList;</span></span>
<span class="line"><span>        }</span></span>
<span class="line"><span>        return serverList;</span></span>
<span class="line"><span>    }</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>    private boolean hasOfflineMachines(List&lt;DynamicServer&gt; serverList) {</span></span>
<span class="line"><span>        if (CollectionUtils.isEmpty(serverList)) {</span></span>
<span class="line"><span>            return false;</span></span>
<span class="line"><span>        }</span></span>
<span class="line"><span>        boolean hasOfflineServer = false;</span></span>
<span class="line"><span>        for (DynamicServer dynamicServer : serverList) {</span></span>
<span class="line"><span>            if (dynamicServer.getOnline() != null &amp;&amp; dynamicServer.getOnline().intValue() == 0) {</span></span>
<span class="line"><span>                hasOfflineServer = true;</span></span>
<span class="line"><span>                break;</span></span>
<span class="line"><span>            }</span></span>
<span class="line"><span>        }</span></span>
<span class="line"><span>        return hasOfflineServer;</span></span>
<span class="line"><span>    }</span></span>
<span class="line"><span> </span></span>
<span class="line"><span>}</span></span></code></pre></div><h3 id="_6-9-网关集群分组隔离" tabindex="-1">6.9 网关集群分组隔离 <a class="header-anchor" href="#_6-9-网关集群分组隔离" aria-label="Permalink to &quot;6.9 网关集群分组隔离&quot;">​</a></h3><blockquote><p>网关集群的分组隔离指的是业务与业务之间的请求应该是隔离的，不应该被部分业务请求打垮了网关服务，从而导致了别的业务请求无法处理。</p></blockquote><p>这里我们会对接接入网关的业务进行分组归类，不同的业务使用不同的分组，不同的网关分组，会部署独立的网关集群，从而隔离了风险，不用再担心业务之间的互相影响。</p><p>举例</p><p>金融业务在生产环境存在一个灰度点检环境，为了配合金融业务的迁移，这边也必须有一套独立的环境为之服务，那是否重新部署一套全新的系统呢(独立的前端+独立的管理后台+独立的网关集群)</p><p>其实不必这么操作，我们只需要部署一套独立的网关集群即可，因为网关管理后台，可以同时配置多个网关分组的数据。</p><p>创建一个新的网关分组finance-gray，而新的网关集群只需要拉取finance-gray分组的配置数据即可，不会对其他网关集群造成任何影响。</p><h2 id="七、-如何快速迁移业务" tabindex="-1">七、.如何快速迁移业务 <a class="header-anchor" href="#七、-如何快速迁移业务" aria-label="Permalink to &quot;七、.如何快速迁移业务&quot;">​</a></h2><p>在业务接入的时候，现有的网关出现了一个尴尬的问题，当某些业务方自行搭建了一套Spring Cloud Gateway网关，里面的服务没有清晰的path前缀、独立的域名拆分，虽然是微服务体系，但是大家共用一个域名，接口前缀也没有良好的划分，混用在一起。</p><p>这个时候如果再按照原有的请求处理流程，则需要业务方进行Nginx的大量修改，需要在location的地方都显式传递serviceName参数，但是业务方不愿意进行这一个调整。</p><p>针对这个问题，其实本质原因在于请求匹配逻辑的不一致性，现有的网关是先匹配服务应用，再进行API匹配，这样效率高一些，而Spring Cloud Gateway则是先API匹配，命中了才知道是哪个后端服务。</p><p>为了解决这个问题，网关再次建立了一个 &quot;微服务集&quot; → &quot;微服务应用列表&quot; 的映射关系，管理后台支持这个映射关系的推送。</p><p>一个网关分组下面会有很多应用服务，这里可以拆分为子集合，可以理解为微服务集就是里面的子集合。</p><p>客户端请求传递过来的时候，需要在请求Header传递 <strong>scTag 参数</strong>，scTag用来标记是哪个微服务集，然后提取到scTag对应的所有后端服务应用列表，依次去对应的应用服务列表中进行API匹配，如果命中了，则代表请求转发到当前应用的后端节点，而对原有的架构改造很小。</p><p>如果不想改动客户端请求，则需要在业务域名的Nginx上进行调整，传递scTag请求Header。</p><h2 id="文章来源" tabindex="-1">文章来源 <a class="header-anchor" href="#文章来源" aria-label="Permalink to &quot;文章来源&quot;">​</a></h2><p>转载说明:</p><ul><li>作者：Lin Chengjun</li><li>版权声明：本文为「 vivo互联网技术」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。</li><li>原文链接：<a href="https://mp.weixin.qq.com/s/5U1rgpcW21LDYzv8K9EX7g" target="_blank" rel="noreferrer">https://mp.weixin.qq.com/s/5U1rgpcW21LDYzv8K9EX7g</a></li></ul><p>本文转自 <a href="https://pdai.tech" target="_blank" rel="noreferrer">https://pdai.tech</a>，如有侵权，请联系删除。</p>`,227)]))}const S=n(m,[["render",y]]);export{H as __pageData,S as default};
