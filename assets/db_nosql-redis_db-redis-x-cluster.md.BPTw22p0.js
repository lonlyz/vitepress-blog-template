import{_ as s,c as a,ai as i,o as l}from"./chunks/framework.BrYByd3F.js";const t="/vitepress-blog-template/images/db/redis/redis-cluster-3.png",r="/vitepress-blog-template/images/db/redis/redis-cluster-4.png",o="/vitepress-blog-template/images/db/redis/redis-cluster-5.png",p="/vitepress-blog-template/images/db/redis/redis-cluster-6.png",n="/vitepress-blog-template/images/db/redis/redis-cluster-7.png",d="/vitepress-blog-template/images/db/redis/redis-cluster-1.png",c="/vitepress-blog-template/images/db/redis/redis-cluster-2.png",h="/vitepress-blog-template/images/db/redis/db-redis-cluster-1.png",u="/vitepress-blog-template/images/db/redis/db-redis-cluster-2.png",g="/vitepress-blog-template/images/db/redis/db-redis-cluster-3.png",R=JSON.parse('{"title":"Redis进阶 - 高可拓展：分片技术（Redis Cluster）详解","description":"","frontmatter":{},"headers":[],"relativePath":"db/nosql-redis/db-redis-x-cluster.md","filePath":"db/nosql-redis/db-redis-x-cluster.md","lastUpdated":1737706346000}'),m={name:"db/nosql-redis/db-redis-x-cluster.md"};function b(k,e,f,v,q,C){return l(),a("div",null,e[0]||(e[0]=[i(`<h1 id="redis进阶-高可拓展-分片技术-redis-cluster-详解" tabindex="-1">Redis进阶 - 高可拓展：分片技术（Redis Cluster）详解 <a class="header-anchor" href="#redis进阶-高可拓展-分片技术-redis-cluster-详解" aria-label="Permalink to &quot;Redis进阶 - 高可拓展：分片技术（Redis Cluster）详解&quot;">​</a></h1><blockquote><p>前面两篇文章，<a href="https://pdai.tech/md/db/nosql-redis/db-redis-x-copy.html" target="_blank" rel="noreferrer">主从复制</a>和<a href="https://pdai.tech/md/db/nosql-redis/db-redis-x-sentinel.html" target="_blank" rel="noreferrer">哨兵机制</a>保障了高可用，就读写分离而言虽然slave节点扩展了主从的读并发能力，但是<strong>写能力</strong>和<strong>存储能力</strong>是无法进行扩展，就只能是master节点能够承载的上限。如果面对海量数据那么必然需要构建master（主节点分片)之间的集群，同时必然需要吸收高可用（主从复制和哨兵机制）能力，即每个master分片节点还需要有slave节点，这是分布式系统中典型的纵向扩展（集群的分片技术）的体现；所以在Redis 3.0版本中对应的设计就是Redis Cluster。@pdai</p></blockquote><h2 id="redis-集群的设计目标" tabindex="-1">Redis 集群的设计目标 <a class="header-anchor" href="#redis-集群的设计目标" aria-label="Permalink to &quot;Redis 集群的设计目标&quot;">​</a></h2><blockquote><p>Redis-cluster是一种服务器Sharding技术，Redis3.0以后版本正式提供支持。Redis Cluster在设计时考虑了什么？我们不妨看下官网的介绍 <a href="https://redis.io/topics/cluster-spec#redis-cluster-goals" target="_blank" rel="noreferrer">Redis Cluster Specification在新窗口打开</a></p></blockquote><h3 id="redis-cluster-goals" tabindex="-1">Redis Cluster goals <a class="header-anchor" href="#redis-cluster-goals" aria-label="Permalink to &quot;Redis Cluster goals&quot;">​</a></h3><p>高性能可线性扩展至最多1000节点。集群中没有代理，（集群节点间）使用异步复制，没有归并操作(merge operations on values)</p><p><strong>可接受的写入安全</strong>:系统尝试(采用best-effort方式)保留所有连接到master节点的client发起的写操作。通常会有一个小的时间窗，时间窗内的已确认写操作可能丢失(即，在发生failover之前的小段时间窗内的写操作可能在failover中丢失)。而在(网络)分区故障下，对少数派master的写入，发生写丢失的时间窗会很大。</p><p><strong>可用性</strong>：Redis Cluster在以下场景下集群总是可用：大部分master节点可用，并且对少部分不可用的master，每一个master至少有一个当前可用的slave。更进一步，通过使用 replicas migration 技术，当前没有slave的master会从当前拥有多个slave的master接受到一个新slave来确保可用性。</p><h3 id="clients-and-servers-roles-in-the-redis-cluster-protocol" tabindex="-1">Clients and Servers roles in the Redis Cluster protocol <a class="header-anchor" href="#clients-and-servers-roles-in-the-redis-cluster-protocol" aria-label="Permalink to &quot;Clients and Servers roles in the Redis Cluster protocol&quot;">​</a></h3><p>Redis Cluster的节点负责维护数据，和获取集群状态，这包括将keys映射到正确的节点。集群节点同样可以自动发现其他节点、检测不工作节点、以及在发现故障发生时晋升slave节点到master</p><p>所有集群节点通过由TCP和二进制协议组成的称为 Redis Cluster Bus 的方式来实现集群的节点自动发现、故障节点探测、slave升级为master等任务。每个节点通过cluster bus连接所有其他节点。节点间使用<strong>gossip协议</strong>进行集群信息传播，以此来实现新节点发现，发送ping包以确认对端工作正常，以及发送cluster消息用来标记特定状态。cluster bus还被用来在集群中创博Pub/Sub消息，以及在接收到用户请求后编排手动failover。</p><h3 id="write-safety" tabindex="-1">Write safety <a class="header-anchor" href="#write-safety" aria-label="Permalink to &quot;Write safety&quot;">​</a></h3><p>Redis Cluster在节点间采用了异步复制，以及 last failover wins 隐含合并功能(implicit merge function)（【译注】不存在合并功能，而是总是认为最近一次failover的节点是最新的）。这意味着最后被选举出的master所包含的数据最终会替代（同一前master下）所有其他备份(replicas/slaves)节点（包含的数据）。当发生分区问题时，总是会有一个时间窗内会发生写入丢失。然而，对连接到多数派master（majority of masters）的client，以及连接到少数派master（mimority of masters）的client，这个时间窗是不同的。</p><p>相比较连接到少数master(minority of masters)的client，对连接到多数master(majority of masters)的client发起的写入，Redis cluster会更努力地尝试将其保存。 下面的场景将会导致在主分区的master上，已经确认的写入在故障期间发生丢失：</p><p>写入请求达到master，但是当master执行完并回复client时，写操作可能还没有通过异步复制传播到它的slave。如果master在写操作抵达slave之前挂了，并且master无法触达(unreachable)的时间足够长而导致了slave节点晋升，那么这个写操作就永远地丢失了。通常很难直接观察到，因为master尝试回复client(写入确认)和传播写操作到slave通常几乎是同时发生。然而，这却是真实世界中的故障方式。（【译注】不考虑返回后宕机的场景，因为宕机导致的写入丢失，在单机版redis上同样存在，这不是redis cluster引入的目的及要解决的问题）</p><p>另一种理论上可能发生写入丢失的模式是：</p><ul><li>master因为分区原因不可用（unreachable）</li><li>该master被某个slave替换(failover)</li><li>一段时间后，该master重新可用</li><li>在该old master变为slave之前，一个client通过过期的路由表对该节点进行写入。</li></ul><p>上述第二种失败场景通常难以发生，因为：</p><ul><li>少数派master(minority master)无法与多数派master(majority master)通信达到一定的时间后，它将拒绝写入，并且当分区恢复后，该master在重新与多数派master建立连接后，还将保持拒绝写入状态一小段时间来感知集群配置变化。留给client可写入的时间窗很小。</li><li>发生这种错误还有一个前提是，client一直都在使用过期的路由表（而实际上集群因为发生了failover，已有slave发生了晋升）。</li></ul><p>写入少数派master(minority side of a partition)会有一个更长的时间窗会导致数据丢失。因为如果最终导致了failover，则写入少数派master的数据将会被多数派一侧(majority side)覆盖（在少数派master作为slave重新接入集群后）。</p><p>特别地，如果要发生failover，master必须至少在NODE_TIMEOUT时间内无法被多数masters(majority of maters)连接，因此如果分区在这一时间内被修复，则不会发生写入丢失。当分区持续时间超过NODE_TIMEOUT时，所有在这段时间内对少数派master(minority side)的写入将会丢失。然而少数派一侧(minority side)将会在NODE_TIMEOUT时间之后如果还没有连上多数派一侧，则它会立即开始拒绝写入，因此对少数派master而言，存在一个进入不可用状态的最大时间窗。在这一时间窗之外，不会再有写入被接受或丢失。</p><h3 id="可用性-availability" tabindex="-1">可用性(Availability) <a class="header-anchor" href="#可用性-availability" aria-label="Permalink to &quot;可用性(Availability)&quot;">​</a></h3><p>Redis Cluster在少数派分区侧不可用。在多数派分区侧，假设由多数派masters存在并且不可达的master有一个slave，cluster将会在NODE_TIMEOUT外加重新选举所需的一小段时间(通常1～2秒)后恢复可用。</p><p>这意味着，Redis Cluster被设计为可以忍受一小部分节点的故障，但是如果需要在大网络分裂(network splits)事件中(【译注】比如发生多分区故障导致网络被分割成多块，且不存在多数派master分区)保持可用性，它不是一个合适的方案(【译注】比如，不要尝试在多机房间部署redis cluster，这不是redis cluster该做的事)。</p><p>假设一个cluster由N个master节点组成并且每个节点仅拥有一个slave，在多数侧只有一个节点出现分区问题时，cluster的多数侧(majority side)可以保持可用，而当有两个节点出现分区故障时，只有 1-(1/(N_2-1)) 的可能性保持集群可用。 也就是说，如果有一个由5个master和5个slave组成的cluster，那么当两个节点出现分区故障时，它有 1/(5_2-1)=11.11%的可能性发生集群不可用。</p><p>Redis cluster提供了一种成为 Replicas Migration 的有用特性特性，它通过自动转移备份节点到孤master节点，在真实世界的常见场景中提升了cluster的可用性。在每次成功的failover之后，cluster会自动重新配置slave分布以尽可能保证在下一次failure中拥有更好的抵御力。</p><h3 id="性能-performance" tabindex="-1">性能(Performance) <a class="header-anchor" href="#性能-performance" aria-label="Permalink to &quot;性能(Performance)&quot;">​</a></h3><p>Redis Cluster不会将命令路由到其中的key所在的节点，而是向client发一个重定向命令 (- MOVED) 引导client到正确的节点。 最终client会获得一个最新的cluster(hash slots分布)展示，以及哪个节点服务于命令中的keys，因此clients就可以获得正确的节点并用来继续执行命令。</p><p>因为master和slave之间使用异步复制，节点不需要等待其他节点对写入的确认（除非使用了WAIT命令）就可以回复client。 同样，因为multi-key命令被限制在了临近的key(near keys)(【译注】即同一hash slot内的key，或者从实际使用场景来说，更多的是通过hash tag定义为具备相同hash字段的有相近业务含义的一组keys)，所以除非触发resharding，数据永远不会在节点间移动。</p><p>普通的命令(normal operations)会像在单个redis实例那样被执行。这意味着一个拥有N个master节点的Redis Cluster，你可以认为它拥有N倍的单个Redis性能。同时，query通常都在一个round trip中执行，因为client通常会保留与所有节点的持久化连接（连接池），因此延迟也与客户端操作单台redis实例没有区别。</p><p>在对数据安全性、可用性方面提供了合理的弱保证的前提下，提供极高的性能和可扩展性，这是Redis Cluster的主要目标。</p><h3 id="避免合并-merge-操作" tabindex="-1">避免合并(merge)操作 <a class="header-anchor" href="#避免合并-merge-操作" aria-label="Permalink to &quot;避免合并(merge)操作&quot;">​</a></h3><p>Redis Cluster设计上避免了在多个拥有相同key-value对的节点上的版本冲突（及合并/merge），因为在redis数据模型下这是不需要的。Redis的值同时都非常大；一个拥有数百万元素的list或sorted set是很常见的。同样，数据类型的语义也很复杂。传输和合并这类值将会产生明显的瓶颈，并可能需要对应用侧的逻辑做明显的修改，比如需要更多的内存来保存meta-data等。</p><p>这里(【译注】刻意避免了merge)并没有严格的技术限制。CRDTs或同步复制状态机可以塑造与redis类似的复杂的数据类型。然而，这类系统运行时的行为与Redis Cluster其实是不一样的。Redis Cluster被设计用来支持非集群redis版本无法支持的一些额外的场景。</p><h2 id="主要模块介绍" tabindex="-1">主要模块介绍 <a class="header-anchor" href="#主要模块介绍" aria-label="Permalink to &quot;主要模块介绍&quot;">​</a></h2><blockquote><p><a href="https://redis.io/topics/cluster-spec#redis-cluster-goals" target="_blank" rel="noreferrer">Redis Cluster Specification在新窗口打开</a>同时还介绍了Redis Cluster中主要模块，这里面包含了很多基础和概念，我们需要先了解下。</p></blockquote><h3 id="哈希槽-hash-slot" tabindex="-1">哈希槽(Hash Slot) <a class="header-anchor" href="#哈希槽-hash-slot" aria-label="Permalink to &quot;哈希槽(Hash Slot)&quot;">​</a></h3><p>Redis-cluster没有使用一致性hash，而是引入了<strong>哈希槽</strong>的概念。Redis-cluster中有16384(即2的14次方）个哈希槽，每个key通过CRC16校验后对16383取模来决定放置哪个槽。Cluster中的每个节点负责一部分hash槽（hash slot）。</p><p>比如集群中存在三个节点，则可能存在的一种分配如下：</p><ul><li>节点A包含0到5500号哈希槽；</li><li>节点B包含5501到11000号哈希槽；</li><li>节点C包含11001 到 16384号哈希槽。</li></ul><h3 id="keys-hash-tags" tabindex="-1">Keys hash tags <a class="header-anchor" href="#keys-hash-tags" aria-label="Permalink to &quot;Keys hash tags&quot;">​</a></h3><p>Hash tags提供了一种途径，<strong>用来将多个(相关的)key分配到相同的hash slot中</strong>。这时Redis Cluster中实现multi-key操作的基础。</p><p>hash tag规则如下，如果满足如下规则，{和}之间的字符将用来计算HASH_SLOT，以保证这样的key保存在同一个slot中。</p><ul><li>key包含一个{字符</li><li>并且 如果在这个{的右面有一个}字符</li><li>并且 如果在{和}之间存在至少一个字符</li></ul><p>例如：</p><ul><li>{user1000}.following和{user1000}.followers这两个key会被hash到相同的hash slot中，因为只有user1000会被用来计算hash slot值。</li><li>foo{}{bar}这个key不会启用hash tag因为第一个{和}之间没有字符。</li><li>foozap这个key中的{bar部分会被用来计算hash slot</li><li>foo{bar}{zap}这个key中的bar会被用来计算计算hash slot，而zap不会</li></ul><h3 id="cluster-nodes属性" tabindex="-1">Cluster nodes属性 <a class="header-anchor" href="#cluster-nodes属性" aria-label="Permalink to &quot;Cluster nodes属性&quot;">​</a></h3><p>每个<strong>节点在cluster中有一个唯一的名字</strong>。这个名字由160bit随机十六进制数字表示，并在节点启动时第一次获得(通常通过/dev/urandom)。节点在配置文件中保留它的ID，并永远地使用这个ID，直到被管理员使用CLUSTER RESET HARD命令hard reset这个节点。</p><p>节点ID被用来在整个cluster中标识每个节点。一个节点可以修改自己的IP地址而不需要修改自己的ID。Cluster可以检测到IP /port的改动并通过运行在cluster bus上的gossip协议重新配置该节点。</p><p>节点ID不是唯一与节点绑定的信息，但是他是唯一的一个总是保持全局一致的字段。每个节点都拥有一系列相关的信息。一些信息时关于本节点在集群中配置细节，并最终在cluster内部保持一致的。而其他信息，比如节点最后被ping的时间，是节点的本地信息。</p><p>每个节点维护着集群内其他节点的以下信息：<code>node id</code>, <code>节点的IP和port</code>，<code>节点标签</code>，<code>master node id</code>（如果这是一个slave节点），<code>最后被挂起的ping的发送时间</code>(如果没有挂起的ping则为0)，<code>最后一次收到pong的时间</code>，<code>当前的节点configuration epoch</code> ，<code>链接状态</code>，以及最后是该节点服务的<code>hash slots</code>。</p><p>对节点字段更详细的描述，可以参考对命令 CLUSTER NODES的描述。</p><p>CLUSTER NODES命令可以被发送到集群内的任意节点，他会提供基于该节点视角(view)下的集群状态以及每个节点的信息。</p><p>下面是一个发送到一个拥有3个节点的小集群的master节点的CLUSTER NODES输出的例子。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>$ redis-cli cluster nodes</span></span>
<span class="line"><span></span></span>
<span class="line"><span>d1861060fe6a534d42d8a19aeb36600e18785e04 127.0.0.1:6379 myself - 0 1318428930 1 connected 0-1364</span></span>
<span class="line"><span>3886e65cc906bfd9b1f7e7bde468726a052d1dae 127.0.0.1:6380 master - 1318428930 1318428931 2 connected 1365-2729</span></span>
<span class="line"><span>d289c575dcbc4bdd2931585fd4339089e461a27d 127.0.0.1:6381 master - 1318428931 1318428931 3 connected 2730-4095</span></span></code></pre></div><p>在上面的例子中，按顺序列出了不同的字段：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>node id, address:port, flags, last ping sent, last pong received, configuration epoch, link state, slots.</span></span></code></pre></div><h3 id="cluster总线" tabindex="-1">Cluster总线 <a class="header-anchor" href="#cluster总线" aria-label="Permalink to &quot;Cluster总线&quot;">​</a></h3><p>每个Redis Cluster节点有一个额外的TCP端口用来接受其他节点的连接。这个端口与用来接收client命令的普通TCP端口有一个固定的offset。该端口等于普通命令端口加上10000.例如，一个Redis街道口在端口6379坚挺客户端连接，那么它的集群总线端口16379也会被打开。</p><p>节点到节点的通讯只使用集群总线，同时使用集群总线协议：有不同的类型和大小的帧组成的二进制协议。集群总线的二进制协议没有被公开文档话，因为他不希望被外部软件设备用来预计群姐点进行对话。</p><h3 id="集群拓扑" tabindex="-1">集群拓扑 <a class="header-anchor" href="#集群拓扑" aria-label="Permalink to &quot;集群拓扑&quot;">​</a></h3><p>Redis Cluster是一张全网拓扑，节点与其他每个节点之间都保持着TCP连接。 在一个拥有N个节点的集群中，每个节点由N-1个TCP传出连接，和N-1个TCP传入连接。 这些TCP连接总是保持活性(be kept alive)。当一个节点在集群总线上发送了ping请求并期待对方回复pong，（如果没有得到回复）在等待足够成时间以便将对方标记为不可达之前，它将先尝试重新连接对方以刷新与对方的连接。 而在全网拓扑中的Redis Cluster节点，节点使用gossip协议和配置更新机制来避免在正常情况下节点之间交换过多的消息，因此集群内交换的消息数目(相对节点数目)不是指数级的。</p><h3 id="节点握手" tabindex="-1">节点握手 <a class="header-anchor" href="#节点握手" aria-label="Permalink to &quot;节点握手&quot;">​</a></h3><p>节点总是接受集群总线端口的链接，并且总是会回复ping请求，即使ping来自一个不可信节点。然而，如果发送节点被认为不是当前集群的一部分，所有其他包将被抛弃。</p><p>节点认定其他节点是当前集群的一部分有两种方式：</p><ol><li>如果一个节点出现在了一条MEET消息中。一条meet消息非常像一个PING消息，但是它会强制接收者接受一个节点作为集群的一部分。节点只有在接收到系统管理员的如下命令后，才会向其他节点发送MEET消息：</li></ol><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>CLUSTER MEET ip port</span></span></code></pre></div><ol start="2"><li>如果一个被信任的节点gossip了某个节点，那么接收到gossip消息的节点也会那个节点标记为集群的一部分。也就是说，如果在集群中，A知道B，而B知道C，最终B会发送gossip消息到A，告诉A节点C是集群的一部分。这时，A会把C注册未网络的一部分，并尝试与C建立连接。</li></ol><p>这意味着，一旦我们把某个节点加入了连接图(connected graph)，它们最终会自动形成一张全连接图(fully connected graph)。这意味着只要系统管理员强制加入了一条信任关系（在某个节点上通过meet命令加入了一个新节点），集群可以自动发现其他节点。</p><h2 id="请求重定向" tabindex="-1">请求重定向 <a class="header-anchor" href="#请求重定向" aria-label="Permalink to &quot;请求重定向&quot;">​</a></h2><blockquote><p>Redis cluster采用去中心化的架构，集群的主节点各自负责一部分槽，客户端如何确定key到底会映射到哪个节点上呢？这就是我们要讲的请求重定向。</p></blockquote><p>在cluster模式下，<strong>节点对请求的处理过程</strong>如下：</p><ul><li>检查当前key是否存在当前NODE？ <ul><li>通过crc16（key）/16384计算出slot</li><li>查询负责该slot负责的节点，得到节点指针</li><li>该指针与自身节点比较</li></ul></li><li>若slot不是由自身负责，则返回MOVED重定向</li><li>若slot由自身负责，且key在slot中，则返回该key对应结果</li><li>若key不存在此slot中，检查该slot是否正在迁出（MIGRATING）？</li><li>若key正在迁出，返回ASK错误重定向客户端到迁移的目的服务器上</li><li>若Slot未迁出，检查Slot是否导入中？</li><li>若Slot导入中且有ASKING标记，则直接操作</li><li>否则返回MOVED重定向</li></ul><p>这个过程中有两点需要具体理解下： <strong>MOVED重定向</strong> 和 <strong>ASK重定向</strong>。</p><h3 id="moved-重定向" tabindex="-1">Moved 重定向 <a class="header-anchor" href="#moved-重定向" aria-label="Permalink to &quot;Moved 重定向&quot;">​</a></h3><p><img src="`+t+'" alt="error.图片加载失败"></p><ul><li>槽命中：直接返回结果</li><li>槽不命中：即当前键命令所请求的键不在当前请求的节点中，则当前节点会向客户端发送一个Moved 重定向，客户端根据Moved 重定向所包含的内容找到目标节点，再一次发送命令。</li></ul><p>从下面可以看出 php 的槽位9244不在当前节点中，所以会重定向到节点 192.168.2.23:7001中。redis-cli会帮你自动重定向（如果没有集群方式启动，即没加参数 -c，redis-cli不会自动重定向），并且编写程序时，寻找目标节点的逻辑需要交予程序员手动完成。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>cluster keyslot keyName # 得到keyName的槽</span></span></code></pre></div><p><img src="'+r+'" alt="error.图片加载失败"></p><h3 id="ask-重定向" tabindex="-1">ASK 重定向 <a class="header-anchor" href="#ask-重定向" aria-label="Permalink to &quot;ASK 重定向&quot;">​</a></h3><p>Ask重定向发生于集群伸缩时，集群伸缩会导致槽迁移，当我们去源节点访问时，此时数据已经可能已经迁移到了目标节点，使用Ask重定向来解决此种情况。</p><p><img src="'+o+'" alt="error.图片加载失败"></p><h3 id="smart客户端" tabindex="-1">smart客户端 <a class="header-anchor" href="#smart客户端" aria-label="Permalink to &quot;smart客户端&quot;">​</a></h3><p>上述两种重定向的机制使得客户端的实现更加复杂，提供了smart客户端（JedisCluster）来<strong>减低复杂性，追求更好的性能</strong>。客户端内部负责计算/维护键-&gt; 槽 -&gt; 节点映射，用于快速定位目标节点。</p><p>实现原理：</p><ul><li>从集群中选取一个可运行节点，使用 cluster slots得到槽和节点的映射关系</li></ul><p><img src="'+p+'" alt="error.图片加载失败"></p><ul><li><p>将上述映射关系存到本地，通过映射关系就可以直接对目标节点进行操作（CRC16(key) -&gt; slot -&gt; node），很好地避免了Moved重定向，并为每个节点创建JedisPool</p></li><li><p>至此就可以用来进行命令操作</p></li></ul><p><img src="'+n+'" alt="error.图片加载失败"></p><h2 id="状态检测及维护" tabindex="-1">状态检测及维护 <a class="header-anchor" href="#状态检测及维护" aria-label="Permalink to &quot;状态检测及维护&quot;">​</a></h2><blockquote><p>Redis Cluster中节点状态如何维护呢？这里便涉及 <strong>有哪些状态</strong>，<strong>底层协议Gossip</strong>，及<strong>具体的通讯（心跳）机制</strong>。@pdai</p></blockquote><p>Cluster中的每个节点都维护一份在自己看来当前整个集群的状态，主要包括：</p><ul><li>当前集群状态</li><li>集群中各节点所负责的slots信息，及其migrate状态</li><li>集群中各节点的master-slave状态</li><li>集群中各节点的存活状态及不可达投票</li></ul><p>当集群状态变化时，如<code>新节点加入</code>、<code>slot迁移</code>、<code>节点宕机</code>、<code>slave提升为新Master</code>，我们希望这些变化尽快的被发现，传播到整个集群的所有节点并达成一致。节点之间相互的<strong>心跳</strong>（PING，PONG，MEET）及其携带的数据是集群状态传播最主要的途径。</p><h3 id="gossip协议" tabindex="-1">Gossip协议 <a class="header-anchor" href="#gossip协议" aria-label="Permalink to &quot;Gossip协议&quot;">​</a></h3><blockquote><p>Redis Cluster 通讯底层是Gossip协议，所以需要对Gossip协议有一定的了解。</p></blockquote><p>gossip 协议（gossip protocol）又称 epidemic 协议（epidemic protocol），是基于流行病传播方式的节点或者进程之间信息交换的协议。 在分布式系统中被广泛使用，比如我们可以使用 gossip 协议来确保网络中所有节点的数据一样。</p><p>Gossip协议已经是P2P网络中比较成熟的协议了。Gossip协议的最大的好处是，<strong>即使集群节点的数量增加，每个节点的负载也不会增加很多，几乎是恒定的。这就允许Consul管理的集群规模能横向扩展到数千个节点</strong>。</p><p>Gossip算法又被称为反熵（Anti-Entropy），熵是物理学上的一个概念，代表杂乱无章，而反熵就是在杂乱无章中寻求一致，这充分说明了Gossip的特点：在一个有界网络中，每个节点都随机地与其他节点通信，经过一番杂乱无章的通信，最终所有节点的状态都会达成一致。每个节点可能知道所有其他节点，也可能仅知道几个邻居节点，只要这些节可以通过网络连通，最终他们的状态都是一致的，当然这也是疫情传播的特点。<a href="https://www.backendcloud.cn/2017/11/12/raft-gossip/" target="_blank" rel="noreferrer">https://www.backendcloud.cn/2017/11/12/raft-gossip/</a></p><p>上面的描述都比较学术，其实Gossip协议对于我们吃瓜群众来说一点也不陌生，Gossip协议也成为流言协议，说白了就是八卦协议，这种传播规模和传播速度都是非常快的，你可以体会一下。所以计算机中的很多算法都是源自生活，而又高于生活的。</p><h4 id="gossip协议的使用" tabindex="-1">Gossip协议的使用 <a class="header-anchor" href="#gossip协议的使用" aria-label="Permalink to &quot;Gossip协议的使用&quot;">​</a></h4><p>Redis 集群是去中心化的，彼此之间状态同步靠 gossip 协议通信，集群的消息有以下几种类型：</p><ul><li><code>Meet</code> 通过「cluster meet ip port」命令，已有集群的节点会向新的节点发送邀请，加入现有集群。</li><li><code>Ping</code> 节点每秒会向集群中其他节点发送 ping 消息，消息中带有自己已知的两个节点的地址、槽、状态信息、最后一次通信时间等。</li><li><code>Pong</code> 节点收到 ping 消息后会回复 pong 消息，消息中同样带有自己已知的两个节点信息。</li><li><code>Fail</code> 节点 ping 不通某节点后，会向集群所有节点广播该节点挂掉的消息。其他节点收到消息后标记已下线。</li></ul><h4 id="基于gossip协议的故障检测" tabindex="-1">基于Gossip协议的故障检测 <a class="header-anchor" href="#基于gossip协议的故障检测" aria-label="Permalink to &quot;基于Gossip协议的故障检测&quot;">​</a></h4><p>集群中的每个节点都会定期地向集群中的其他节点发送PING消息，以此交换各个节点状态信息，检测各个节点状态：<strong>在线状态</strong>、<strong>疑似下线状态PFAIL</strong>、<strong>已下线状态FAIL</strong>。</p><p><strong>自己保存信息</strong>：当主节点A通过消息得知主节点B认为主节点D进入了疑似下线(PFAIL)状态时,主节点A会在自己的clusterState.nodes字典中找到主节点D所对应的clusterNode结构，并将主节点B的下线报告添加到clusterNode结构的fail_reports链表中，并后续关于结点D疑似下线的状态通过Gossip协议通知其他节点。</p><p><strong>一起裁定</strong>：如果集群里面，半数以上的主节点都将主节点D报告为疑似下线，那么主节点D将被标记为已下线(FAIL)状态，将主节点D标记为已下线的节点会向集群广播主节点D的FAIL消息，所有收到FAIL消息的节点都会立即更新nodes里面主节点D状态标记为已下线。</p><p><strong>最终裁定</strong>：将 node 标记为 FAIL 需要满足以下两个条件：</p><ul><li>有半数以上的主节点将 node 标记为 PFAIL 状态。</li><li>当前节点也将 node 标记为 PFAIL 状态。</li></ul><h3 id="通讯状态和维护" tabindex="-1">通讯状态和维护 <a class="header-anchor" href="#通讯状态和维护" aria-label="Permalink to &quot;通讯状态和维护&quot;">​</a></h3><blockquote><p>我们理解了Gossip协议基础后，就可以进一步理解Redis节点之间相互的通讯<strong>心跳</strong>（PING，PONG，MEET）实现和维护了。我们通过几个问题来具体理解。</p></blockquote><h4 id="什么时候进行心跳" tabindex="-1">什么时候进行心跳？ <a class="header-anchor" href="#什么时候进行心跳" aria-label="Permalink to &quot;什么时候进行心跳？&quot;">​</a></h4><p>Redis节点会记录其向每一个节点上一次发出ping和收到pong的时间，心跳发送时机与这两个值有关。通过下面的方式既能保证及时更新集群状态，又不至于使心跳数过多：</p><ul><li>每次Cron向所有未建立链接的节点发送ping或meet</li><li>每1秒从所有已知节点中随机选取5个，向其中上次收到pong最久远的一个发送ping</li><li>每次Cron向收到pong超过timeout/2的节点发送ping</li><li>收到ping或meet，立即回复pong</li></ul><h4 id="发送哪些心跳数据" tabindex="-1">发送哪些心跳数据？ <a class="header-anchor" href="#发送哪些心跳数据" aria-label="Permalink to &quot;发送哪些心跳数据？&quot;">​</a></h4><ul><li>Header，发送者自己的信息 <ul><li>所负责slots的信息</li><li>主从信息</li><li>ip port信息</li><li>状态信息</li></ul></li><li>Gossip，发送者所了解的部分其他节点的信息 <ul><li>ping_sent, pong_received</li><li>ip, port信息</li><li>状态信息，比如发送者认为该节点已经不可达，会在状态信息中标记其为PFAIL或FAIL</li></ul></li></ul><h4 id="如何处理心跳" tabindex="-1">如何处理心跳？ <a class="header-anchor" href="#如何处理心跳" aria-label="Permalink to &quot;如何处理心跳？&quot;">​</a></h4><p>1，<strong>新节点加入</strong></p><ul><li>发送meet包加入集群</li><li>从pong包中的gossip得到未知的其他节点</li><li>循环上述过程，直到最终加入集群</li></ul><p><img src="'+d+'" alt="error.图片加载失败"></p><p>2，<strong>Slots信息</strong></p><ul><li>判断发送者声明的slots信息，跟本地记录的是否有不同</li><li>如果不同，且发送者epoch较大，更新本地记录</li><li>如果不同，且发送者epoch小，发送Update信息通知发送者</li></ul><p>3，<strong>Master slave信息</strong></p><p>发现发送者的master、slave信息变化，更新本地状态</p><p>4，<strong>节点Fail探测(故障发现)</strong></p><ul><li>超过超时时间仍然没有收到pong包的节点会被当前节点标记为PFAIL</li><li>PFAIL标记会随着gossip传播</li><li>每次收到心跳包会检测其中对其他节点的PFAIL标记，当做对该节点FAIL的投票维护在本机</li><li>对某个节点的PFAIL标记达到大多数时，将其变为FAIL标记并广播FAIL消息</li></ul><blockquote><p>注：Gossip的存在使得集群状态的改变可以更快的达到整个集群。每个心跳包中会包含多个Gossip包，那么多少个才是合适的呢，redis的选择是N/10，其中N是节点数，这样可以保证在PFAIL投票的过期时间内，节点可以收到80%机器关于失败节点的gossip，从而使其顺利进入FAIL状态。</p></blockquote><h4 id="将信息广播给其它节点" tabindex="-1">将信息广播给其它节点？ <a class="header-anchor" href="#将信息广播给其它节点" aria-label="Permalink to &quot;将信息广播给其它节点？&quot;">​</a></h4><p>当需要发布一些非常重要需要立即送达的信息时，上述心跳加Gossip的方式就显得捉襟见肘了，这时就需要向所有集群内机器的广播信息，使用广播发的场景：</p><ul><li><strong>节点的Fail信息</strong>：当发现某一节点不可达时，探测节点会将其标记为PFAIL状态，并通过心跳传播出去。当某一节点发现这个节点的PFAIL超过半数时修改其为FAIL并发起广播。</li><li><strong>Failover Request信息</strong>：slave尝试发起FailOver时广播其要求投票的信息</li><li><strong>新Master信息</strong>：Failover成功的节点向整个集群广播自己的信息</li></ul><h2 id="故障恢复-failover" tabindex="-1">故障恢复（Failover） <a class="header-anchor" href="#故障恢复-failover" aria-label="Permalink to &quot;故障恢复（Failover）&quot;">​</a></h2><blockquote><p>master节点挂了之后，如何进行故障恢复呢？</p></blockquote><p>当slave发现自己的master变为FAIL状态时，便尝试进行Failover，以期成为新的master。由于挂掉的master可能会有多个slave。Failover的过程需要经过类Raft协议的过程在整个集群内达到一致， 其过程如下：</p><ul><li>slave发现自己的master变为FAIL</li><li>将自己记录的集群currentEpoch加1，并广播Failover Request信息</li><li>其他节点收到该信息，只有master响应，判断请求者的合法性，并发送FAILOVER_AUTH_ACK，对每一个epoch只发送一次ack</li><li>尝试failover的slave收集FAILOVER_AUTH_ACK</li><li>超过半数后变成新Master</li><li>广播Pong通知其他集群节点</li></ul><p><img src="'+c+`" alt="error.图片加载失败"></p><h2 id="扩容-缩容" tabindex="-1">扩容&amp;缩容 <a class="header-anchor" href="#扩容-缩容" aria-label="Permalink to &quot;扩容&amp;缩容&quot;">​</a></h2><blockquote><p>Redis Cluster是如何进行扩容和缩容的呢？</p></blockquote><h3 id="扩容" tabindex="-1">扩容 <a class="header-anchor" href="#扩容" aria-label="Permalink to &quot;扩容&quot;">​</a></h3><p>当集群出现容量限制或者其他一些原因需要扩容时，redis cluster提供了比较优雅的集群扩容方案。</p><ol><li><p>首先将新节点加入到集群中，可以通过在集群中任何一个客户端执行cluster meet 新节点ip:端口，或者通过redis-trib add node添加，新添加的节点默认在集群中都是主节点。</p></li><li><p>迁移数据 迁移数据的大致流程是，首先需要确定哪些槽需要被迁移到目标节点，然后获取槽中key，将槽中的key全部迁移到目标节点，然后向集群所有主节点广播槽（数据）全部迁移到了目标节点。直接通过redis-trib工具做数据迁移很方便。 现在假设将节点A的槽10迁移到B节点，过程如下：</p></li></ol><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>B:cluster setslot 10 importing A.nodeId</span></span>
<span class="line"><span>A:cluster setslot 10 migrating B.nodeId</span></span></code></pre></div><p>循环获取槽中key，将key迁移到B节点</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>A:cluster getkeysinslot 10 100</span></span>
<span class="line"><span>A:migrate B.ip B.port &quot;&quot; 0 5000 keys key1[ key2....]</span></span></code></pre></div><p>向集群广播槽已经迁移到B节点</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>cluster setslot 10 node B.nodeId</span></span></code></pre></div><h3 id="缩容" tabindex="-1">缩容 <a class="header-anchor" href="#缩容" aria-label="Permalink to &quot;缩容&quot;">​</a></h3><p>缩容的大致过程与扩容一致，需要判断下线的节点是否是主节点，以及主节点上是否有槽，若主节点上有槽，需要将槽迁移到集群中其他主节点，槽迁移完成之后，需要向其他节点广播该节点准备下线（cluster forget nodeId）。最后需要将该下线主节点的从节点指向其他主节点，当然最好是先将从节点下线。</p><h2 id="更深入理解" tabindex="-1">更深入理解 <a class="header-anchor" href="#更深入理解" aria-label="Permalink to &quot;更深入理解&quot;">​</a></h2><blockquote><p>通过几个例子，再深入理解Redis Cluster</p></blockquote><h3 id="为什么redis-cluster的hash-slot-是16384" tabindex="-1">为什么Redis Cluster的Hash Slot 是16384？ <a class="header-anchor" href="#为什么redis-cluster的hash-slot-是16384" aria-label="Permalink to &quot;为什么Redis Cluster的Hash Slot 是16384？&quot;">​</a></h3><p>我们知道一致性hash算法是2的16次方，为什么hash slot是2的14次方呢？作者<a href="https://github.com/redis/redis/issues/2576" target="_blank" rel="noreferrer">原始回答在新窗口打开</a></p><p>在redis节点发送心跳包时需要把所有的槽放到这个心跳包里，以便让节点知道当前集群信息，16384=16k，在发送心跳包时使用char进行bitmap压缩后是2k（2 * 8 (8 bit) * 1024(1k) = 16K），也就是说使用2k的空间创建了16k的槽数。</p><p>虽然使用CRC16算法最多可以分配65535（2^16-1）个槽位，65535=65k，压缩后就是8k（8 * 8 (8 bit) * 1024(1k) =65K），也就是说需要需要8k的心跳包，作者认为这样做不太值得；并且一般情况下一个redis集群不会有超过1000个master节点，所以16k的槽位是个比较合适的选择。</p><h3 id="为什么redis-cluster中不建议使用发布订阅呢" tabindex="-1">为什么Redis Cluster中不建议使用发布订阅呢？ <a class="header-anchor" href="#为什么redis-cluster中不建议使用发布订阅呢" aria-label="Permalink to &quot;为什么Redis Cluster中不建议使用发布订阅呢？&quot;">​</a></h3><p>在集群模式下，所有的publish命令都会向所有节点（包括从节点）进行广播，造成每条publish数据都会在集群内所有节点传播一次，加重了带宽负担，对于在有大量节点的集群中频繁使用pub，会严重消耗带宽，不建议使用。（虽然官网上讲有时候可以使用Bloom过滤器或其他算法进行优化的）</p><h2 id="其它常见方案" tabindex="-1">其它常见方案 <a class="header-anchor" href="#其它常见方案" aria-label="Permalink to &quot;其它常见方案&quot;">​</a></h2><blockquote><p>还有一些方案出现在历史舞台上，我挑了几个经典的。简单了解下，增强下关联的知识体系。@pdai</p></blockquote><h3 id="redis-sentinel-集群-keepalived-haproxy" tabindex="-1">Redis Sentinel 集群 + Keepalived/Haproxy <a class="header-anchor" href="#redis-sentinel-集群-keepalived-haproxy" aria-label="Permalink to &quot;Redis Sentinel 集群 + Keepalived/Haproxy&quot;">​</a></h3><p>底层是 Redis Sentinel 集群，代理着 Redis 主从，Web 端通过 VIP 提供服务。当主节点发生故障，比如机器故障、Redis 节点故障或者网络不可达，Redis 之间的切换通过 Redis Sentinel 内部机制保障，VIP 切换通过 Keepalived 保障。</p><p><img src="`+h+'" alt="error.图片加载失败"></p><p>优点：</p><ul><li>秒级切换</li><li>对应用透明</li></ul><p>缺点：</p><ul><li>维护成本高</li><li>存在脑裂</li><li>Sentinel 模式存在短时间的服务不可用</li></ul><h3 id="twemproxy" tabindex="-1">Twemproxy <a class="header-anchor" href="#twemproxy" aria-label="Permalink to &quot;Twemproxy&quot;">​</a></h3><p>多个同构 Twemproxy（配置相同）同时工作，接受客户端的请求，根据 hash 算法，转发给对应的 Redis。</p><p>Twemproxy 方案比较成熟了，但是效果并不是很理想。一方面是定位问题比较困难，另一方面是它对自动剔除节点的支持不是很友好。</p><p><img src="'+u+'" alt="error.图片加载失败"></p><p>优点：</p><ul><li>开发简单，对应用几乎透明</li><li>历史悠久，方案成熟</li></ul><p>缺点：</p><ul><li>代理影响性能</li><li>LVS 和 Twemproxy 会有节点性能瓶颈</li><li>Redis 扩容非常麻烦</li><li>Twitter 内部已放弃使用该方案，新使用的架构未开源</li></ul><h3 id="codis" tabindex="-1">Codis <a class="header-anchor" href="#codis" aria-label="Permalink to &quot;Codis&quot;">​</a></h3><p>Codis 是由豌豆荚开源的产品，涉及组件众多，其中 ZooKeeper 存放路由表和代理节点元数据、分发 Codis-Config 的命令；Codis-Config 是集成管理工具，有 Web 界面供使用；Codis-Proxy 是一个兼容 Redis 协议的无状态代理；Codis-Redis 基于 Redis 2.8 版本二次开发，加入 slot 支持，方便迁移数据。</p><p><img src="'+g+'" alt="error.图片加载失败"></p><p>优点：</p><ul><li>开发简单，对应用几乎透明</li><li>性能比 Twemproxy 好</li><li>有图形化界面，扩容容易，运维方便</li></ul><p>缺点：</p><ul><li>代理依旧影响性能</li><li>组件过多，需要很多机器资源</li><li>修改了 Redis 代码，导致和官方无法同步，新特性跟进缓慢</li><li>开发团队准备主推基于 Redis 改造的 reborndb</li></ul><h2 id="参考文章" tabindex="-1">参考文章 <a class="header-anchor" href="#参考文章" aria-label="Permalink to &quot;参考文章&quot;">​</a></h2><ul><li><a href="https://redis.io/topics/cluster-tutorial" target="_blank" rel="noreferrer">https://redis.io/topics/cluster-tutorial</a></li><li><a href="https://www.linuxprobe.com/redis-high-availability.html" target="_blank" rel="noreferrer">https://www.linuxprobe.com/redis-high-availability.html</a></li></ul><p>部分章节参考了</p><ul><li><a href="https://www.jianshu.com/p/2b5c9efdfea6" target="_blank" rel="noreferrer">https://www.jianshu.com/p/2b5c9efdfea6</a></li><li><a href="https://www.jianshu.com/p/87e06d81b597" target="_blank" rel="noreferrer">https://www.jianshu.com/p/87e06d81b597</a></li><li><a href="https://www.jianshu.com/p/bb857f883ccb" target="_blank" rel="noreferrer">https://www.jianshu.com/p/bb857f883ccb</a></li><li><a href="https://zhuanlan.zhihu.com/p/92937061" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/92937061</a></li></ul><p>一些具体的实践可以参考</p><p><a href="https://www.cnblogs.com/kismetv/p/9853040.html" target="_blank" rel="noreferrer">https://www.cnblogs.com/kismetv/p/9853040.html</a></p><p>本文转自 <a href="https://pdai.tech" target="_blank" rel="noreferrer">https://pdai.tech</a>，如有侵权，请联系删除。</p>',187)]))}const P=s(m,[["render",b]]);export{R as __pageData,P as default};
