import{_ as s,c as e,ai as p,o as n}from"./chunks/framework.BrYByd3F.js";const t="/vitepress-blog-template/images/jvm/offheap/jvm-gc-offheap-1.png",r="/vitepress-blog-template/images/jvm/offheap/jvm-gc-offheap-2.png",l="/vitepress-blog-template/images/jvm/offheap/jvm-gc-offheap-3.png",o="/vitepress-blog-template/images/jvm/offheap/jvm-gc-offheap-4.png",i="/vitepress-blog-template/images/jvm/offheap/jvm-gc-offheap-5.jpg",c="/vitepress-blog-template/images/jvm/offheap/jvm-gc-offheap-6.jpg",m="/vitepress-blog-template/images/jvm/offheap/jvm-gc-offheap-7.png",g="/vitepress-blog-template/images/jvm/offheap/jvm-gc-offheap-8.png",f="/vitepress-blog-template/images/jvm/offheap/jvm-gc-offheap-9.png",h="/vitepress-blog-template/images/jvm/offheap/jvm-gc-offheap-10.png",d="/vitepress-blog-template/images/jvm/offheap/jvm-gc-offheap-11.png",v="/vitepress-blog-template/images/jvm/offheap/jvm-gc-offheap-12.jpg",b="/vitepress-blog-template/images/jvm/offheap/jvm-gc-offheap-13.jpg",u="/vitepress-blog-template/images/jvm/offheap/jvm-gc-offheap-14.jpg",A=JSON.parse('{"title":"调试排错 - Java 内存分析之堆外内存","description":"","frontmatter":{},"headers":[],"relativePath":"java/jvm/java-jvm-oom-offheap.md","filePath":"java/jvm/java-jvm-oom-offheap.md","lastUpdated":1737706346000}'),_={name:"java/jvm/java-jvm-oom-offheap.md"};function j(k,a,M,C,z,S){return n(),e("div",null,a[0]||(a[0]=[p('<h1 id="调试排错-java-内存分析之堆外内存" tabindex="-1">调试排错 - Java 内存分析之堆外内存 <a class="header-anchor" href="#调试排错-java-内存分析之堆外内存" aria-label="Permalink to &quot;调试排错 - Java 内存分析之堆外内存&quot;">​</a></h1><blockquote><p>Java 堆外内存分析相对来说是复杂的，美团技术团队的<a href="https://tech.meituan.com/2019/01/03/spring-boot-native-memory-leak.html" target="_blank" rel="noreferrer">Spring Boot引起的“堆外内存泄漏”排查及经验总结在新窗口打开</a>可以为很多Native Code内存泄漏/占用提供方向性指引。@pdai</p></blockquote><h2 id="背景" tabindex="-1">背景 <a class="header-anchor" href="#背景" aria-label="Permalink to &quot;背景&quot;">​</a></h2><p>为了更好地实现对项目的管理，我们将组内一个项目迁移到MDP框架（基于Spring Boot），随后我们就发现系统会频繁报出Swap区域使用量过高的异常。笔者被叫去帮忙查看原因，发现配置了4G堆内内存，但是实际使用的物理内存竟然高达7G，确实不正常。JVM参数配置是“-XX:MetaspaceSize=256M -XX:MaxMetaspaceSize=256M -XX:+AlwaysPreTouch -XX:ReservedCodeCacheSize=128m -XX:InitialCodeCacheSize=128m, -Xss512k -Xmx4g -Xms4g,-XX:+UseG1GC -XX:G1HeapRegionSize=4M”，实际使用的物理内存如下图所示：</p><p><img src="'+t+'" alt="error.图片加载失败"></p><h2 id="排查过程" tabindex="-1">排查过程 <a class="header-anchor" href="#排查过程" aria-label="Permalink to &quot;排查过程&quot;">​</a></h2><h3 id="使用java层面的工具定位内存区域" tabindex="-1">使用Java层面的工具定位内存区域 <a class="header-anchor" href="#使用java层面的工具定位内存区域" aria-label="Permalink to &quot;使用Java层面的工具定位内存区域&quot;">​</a></h3><blockquote><p>使用Java层面的工具可以定位出堆内内存、Code区域或者使用unsafe.allocateMemory和DirectByteBuffer申请的堆外内存</p></blockquote><p>笔者在项目中添加<code>-XX:NativeMemoryTracking=detailJVM</code>参数重启项目，使用命令<code>jcmd pid VM.native_memory detail</code>查看到的内存分布如下：</p><p><img src="'+r+'" alt="error.图片加载失败"></p><p>发现命令显示的committed的内存小于物理内存，因为jcmd命令显示的内存包含堆内内存、Code区域、通过unsafe.allocateMemory和DirectByteBuffer申请的内存，但是不包含其他Native Code（C代码）申请的堆外内存。所以猜测是使用Native Code申请内存所导致的问题。</p><p>为了防止误判，笔者使用了pmap查看内存分布，发现大量的64M的地址；而这些地址空间不在jcmd命令所给出的地址空间里面，基本上就断定就是这些64M的内存所导致。</p><p><img src="'+l+'" alt="error.图片加载失败"></p><h3 id="使用系统层面的工具定位堆外内存" tabindex="-1">使用系统层面的工具定位堆外内存 <a class="header-anchor" href="#使用系统层面的工具定位堆外内存" aria-label="Permalink to &quot;使用系统层面的工具定位堆外内存&quot;">​</a></h3><p>因为笔者已经基本上确定是Native Code所引起，而Java层面的工具不便于排查此类问题，只能使用系统层面的工具去定位问题。</p><h4 id="首先-使用了gperftools去定位问题" tabindex="-1">首先，使用了gperftools去定位问题 <a class="header-anchor" href="#首先-使用了gperftools去定位问题" aria-label="Permalink to &quot;首先，使用了gperftools去定位问题&quot;">​</a></h4><p>gperftools的使用方法可以参考gperftools，gperftools的监控如下：</p><p><img src="'+o+'" alt="error.图片加载失败"></p><p>从上图可以看出：使用malloc申请的的内存最高到3G之后就释放了，之后始终维持在700M-800M。笔者第一反应是：难道Native Code中没有使用malloc申请，直接使用mmap/brk申请的？（gperftools原理就使用动态链接的方式替换了操作系统默认的内存分配器（glibc）。）</p><h4 id="然后-使用strace去追踪系统调用" tabindex="-1">然后，使用strace去追踪系统调用 <a class="header-anchor" href="#然后-使用strace去追踪系统调用" aria-label="Permalink to &quot;然后，使用strace去追踪系统调用&quot;">​</a></h4><p>因为使用gperftools没有追踪到这些内存，于是直接使用命令“strace -f -e”brk,mmap,munmap” -p pid”追踪向OS申请内存请求，但是并没有发现有可疑内存申请。strace监控如下图所示:</p><p><img src="'+i+'" alt="error.图片加载失败"></p><h4 id="接着-使用gdb去dump可疑内存" tabindex="-1">接着，使用GDB去dump可疑内存 <a class="header-anchor" href="#接着-使用gdb去dump可疑内存" aria-label="Permalink to &quot;接着，使用GDB去dump可疑内存&quot;">​</a></h4><p>因为使用strace没有追踪到可疑内存申请；于是想着看看内存中的情况。就是直接使用命令gdp -pid pid进入GDB之后，然后使用命令dump memory mem.bin startAddress endAddressdump内存，其中startAddress和endAddress可以从/proc/pid/smaps中查找。然后使用strings mem.bin查看dump的内容，如下：</p><p><img src="'+c+'" alt="error.图片加载失败"></p><p>从内容上来看，像是解压后的JAR包信息。读取JAR包信息应该是在项目启动的时候，那么在项目启动之后使用strace作用就不是很大了。所以应该在项目启动的时候使用strace，而不是启动完成之后。</p><h4 id="再次-项目启动时使用strace去追踪系统调用" tabindex="-1">再次，项目启动时使用strace去追踪系统调用 <a class="header-anchor" href="#再次-项目启动时使用strace去追踪系统调用" aria-label="Permalink to &quot;再次，项目启动时使用strace去追踪系统调用&quot;">​</a></h4><p>项目启动使用strace追踪系统调用，发现确实申请了很多64M的内存空间，截图如下：</p><p><img src="'+m+'" alt="error.图片加载失败"></p><p>使用该mmap申请的地址空间在pmap对应如下：</p><p><img src="'+g+'" alt="error.图片加载失败"></p><h4 id="最后-使用jstack去查看对应的线程" tabindex="-1">最后，使用jstack去查看对应的线程 <a class="header-anchor" href="#最后-使用jstack去查看对应的线程" aria-label="Permalink to &quot;最后，使用jstack去查看对应的线程&quot;">​</a></h4><p>因为strace命令中已经显示申请内存的线程ID。直接使用命令jstack pid去查看线程栈，找到对应的线程栈（注意10进制和16进制转换）如下：</p><p><img src="'+f+'" alt="error.图片加载失败"></p><p>这里基本上就可以看出问题来了：MCC（美团统一配置中心）使用了Reflections进行扫包，底层使用了Spring Boot去加载JAR。因为解压JAR使用Inflater类，需要用到堆外内存，然后使用Btrace去追踪这个类，栈如下：</p><p><img src="'+h+'" alt="error.图片加载失败"></p><p>然后查看使用MCC的地方，发现没有配置扫包路径，默认是扫描所有的包。于是修改代码，配置扫包路径，发布上线后内存问题解决。</p><h3 id="为什么堆外内存没有释放掉呢" tabindex="-1">为什么堆外内存没有释放掉呢？ <a class="header-anchor" href="#为什么堆外内存没有释放掉呢" aria-label="Permalink to &quot;为什么堆外内存没有释放掉呢？&quot;">​</a></h3><p>虽然问题已经解决了，但是有几个疑问：</p><ul><li>为什么使用旧的框架没有问题？</li><li>为什么堆外内存没有释放？</li><li>为什么内存大小都是64M，JAR大小不可能这么大，而且都是一样大？</li><li>为什么gperftools最终显示使用的的内存大小是700M左右，解压包真的没有使用malloc申请内存吗？</li></ul><p>带着疑问，笔者直接看了一下<a href="https://github.com/spring-projects/spring-boot/tree/master/spring-boot-project/spring-boot-tools/spring-boot-loader/src/main/java/org/springframework/boot/loader" target="_blank" rel="noreferrer">Spring Boot Loader在新窗口打开</a>那一块的源码。发现Spring Boot对Java JDK的InflaterInputStream进行了包装并且使用了Inflater，而Inflater本身用于解压JAR包的需要用到堆外内存。而包装之后的类ZipInflaterInputStream没有释放Inflater持有的堆外内存。于是笔者以为找到了原因，立马向Spring Boot社区反馈了<a href="https://github.com/spring-projects/spring-boot/issues/13935" target="_blank" rel="noreferrer">这个bug在新窗口打开</a>。但是反馈之后，笔者就发现Inflater这个对象本身实现了finalize方法，在这个方法中有调用释放堆外内存的逻辑。也就是说Spring Boot依赖于GC释放堆外内存。</p><p>笔者使用jmap查看堆内对象时，发现已经基本上没有Inflater这个对象了。于是就怀疑GC的时候，没有调用finalize。带着这样的怀疑，笔者把Inflater进行包装在Spring Boot Loader里面替换成自己包装的Inflater，在finalize进行打点监控，结果finalize方法确实被调用了。于是笔者又去看了Inflater对应的C代码，发现初始化的使用了malloc申请内存，end的时候也调用了free去释放内存。</p><p>此刻，笔者只能怀疑free的时候没有真正释放内存，便把Spring Boot包装的InflaterInputStream替换成Java JDK自带的，发现替换之后，内存问题也得以解决了。</p><p>这时，再返过来看gperftools的内存分布情况，发现使用Spring Boot时，内存使用一直在增加，突然某个点内存使用下降了好多（使用量直接由3G降为700M左右）。这个点应该就是GC引起的，内存应该释放了，但是在操作系统层面并没有看到内存变化，那是不是没有释放到操作系统，被内存分配器持有了呢？</p><p>继续探究，发现系统默认的内存分配器（glibc 2.12版本）和使用gperftools内存地址分布差别很明显，2.5G地址使用smaps发现它是属于Native Stack。内存地址分布如下：</p><p><img src="'+d+'" alt="error.图片加载失败"></p><p>到此，基本上可以确定是内存分配器在捣鬼；搜索了一下glibc 64M，发现glibc从2.11开始对每个线程引入内存池（64位机器大小就是64M内存），原文如下：</p><p><img src="'+v+`" alt="error.图片加载失败"></p><p>按照文中所说去修改MALLOC_ARENA_MAX环境变量，发现没什么效果。查看tcmalloc（gperftools使用的内存分配器）也使用了内存池方式。</p><p>为了验证是内存池搞的鬼，笔者就简单写个不带内存池的内存分配器。使用命令<code>gcc zjbmalloc.c -fPIC -shared -o zjbmalloc.so</code>生成动态库，然后使用<code>export LD_PRELOAD=zjbmalloc.so</code>替换掉glibc的内存分配器。其中代码Demo如下：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>#include&lt;sys/mman.h&gt;</span></span>
<span class="line"><span>#include&lt;stdlib.h&gt;</span></span>
<span class="line"><span>#include&lt;string.h&gt;</span></span>
<span class="line"><span>#include&lt;stdio.h&gt;</span></span>
<span class="line"><span>//作者使用的64位机器，sizeof(size_t)也就是sizeof(long) </span></span>
<span class="line"><span>void* malloc ( size_t size )</span></span>
<span class="line"><span>{</span></span>
<span class="line"><span>   long* ptr = mmap( 0, size + sizeof(long), PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, 0, 0 );</span></span>
<span class="line"><span>   if (ptr == MAP_FAILED) {</span></span>
<span class="line"><span>  	return NULL;</span></span>
<span class="line"><span>   }</span></span>
<span class="line"><span>   *ptr = size;                     // First 8 bytes contain length.</span></span>
<span class="line"><span>   return (void*)(&amp;ptr[1]);        // Memory that is after length variable</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span></span></span>
<span class="line"><span>void *calloc(size_t n, size_t size) {</span></span>
<span class="line"><span> void* ptr = malloc(n * size);</span></span>
<span class="line"><span> if (ptr == NULL) {</span></span>
<span class="line"><span>	return NULL;</span></span>
<span class="line"><span> }</span></span>
<span class="line"><span> memset(ptr, 0, n * size);</span></span>
<span class="line"><span> return ptr;</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span>void *realloc(void *ptr, size_t size)</span></span>
<span class="line"><span>{</span></span>
<span class="line"><span> if (size == 0) {</span></span>
<span class="line"><span>	free(ptr);</span></span>
<span class="line"><span>	return NULL;</span></span>
<span class="line"><span> }</span></span>
<span class="line"><span> if (ptr == NULL) {</span></span>
<span class="line"><span>	return malloc(size);</span></span>
<span class="line"><span> }</span></span>
<span class="line"><span> long *plen = (long*)ptr;</span></span>
<span class="line"><span> plen--;                          // Reach top of memory</span></span>
<span class="line"><span> long len = *plen;</span></span>
<span class="line"><span> if (size &lt;= len) {</span></span>
<span class="line"><span>	return ptr;</span></span>
<span class="line"><span> }</span></span>
<span class="line"><span> void* rptr = malloc(size);</span></span>
<span class="line"><span> if (rptr == NULL) {</span></span>
<span class="line"><span>	free(ptr);</span></span>
<span class="line"><span>	return NULL;</span></span>
<span class="line"><span> }</span></span>
<span class="line"><span> rptr = memcpy(rptr, ptr, len);</span></span>
<span class="line"><span> free(ptr);</span></span>
<span class="line"><span> return rptr;</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span></span></span>
<span class="line"><span>void free (void* ptr )</span></span>
<span class="line"><span>{</span></span>
<span class="line"><span>   if (ptr == NULL) {</span></span>
<span class="line"><span>	 return;</span></span>
<span class="line"><span>   }</span></span>
<span class="line"><span>   long *plen = (long*)ptr;</span></span>
<span class="line"><span>   plen--;                          // Reach top of memory</span></span>
<span class="line"><span>   long len = *plen;               // Read length</span></span>
<span class="line"><span>   munmap((void*)plen, len + sizeof(long));</span></span>
<span class="line"><span>}</span></span></code></pre></div><p>通过在自定义分配器当中埋点可以发现其实程序启动之后应用实际申请的堆外内存始终在700M-800M之间，gperftools监控显示内存使用量也是在700M-800M左右。但是从操作系统角度来看进程占用的内存差别很大（这里只是监控堆外内存）。</p><p>笔者做了一下测试，使用不同分配器进行不同程度的扫包，占用的内存如下：</p><p><img src="`+b+'" alt="error.图片加载失败"></p><p><strong>为什么自定义的malloc申请800M，最终占用的物理内存在1.7G呢</strong>？</p><p>因为自定义内存分配器采用的是mmap分配内存，mmap分配内存按需向上取整到整数个页，所以存在着巨大的空间浪费。通过监控发现最终申请的页面数目在536k个左右，那实际上向系统申请的内存等于512k * 4k（pagesize） = 2G。</p><p><strong>为什么这个数据大于1.7G呢</strong>？</p><p>因为操作系统采取的是延迟分配的方式，通过mmap向系统申请内存的时候，系统仅仅返回内存地址并没有分配真实的物理内存。只有在真正使用的时候，系统产生一个缺页中断，然后再分配实际的物理Page。</p><h2 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-label="Permalink to &quot;总结&quot;">​</a></h2><p><img src="'+u+'" alt="error.图片加载失败"></p><p>整个内存分配的流程如上图所示。MCC扫包的默认配置是扫描所有的JAR包。在扫描包的时候，Spring Boot不会主动去释放堆外内存，导致在扫描阶段，堆外内存占用量一直持续飙升。当发生GC的时候，Spring Boot依赖于finalize机制去释放了堆外内存；但是glibc为了性能考虑，并没有真正把内存归返到操作系统，而是留下来放入内存池了，导致应用层以为发生了“内存泄漏”。所以修改MCC的配置路径为特定的JAR包，问题解决。笔者在发表这篇文章时，发现<strong>Spring Boot的最新版本（2.0.5.RELEASE）已经做了修改，在ZipInflaterInputStream主动释放了堆外内存不再依赖GC</strong>；所以Spring Boot升级到最新版本，这个问题也可以得到解决。</p><h2 id="参考资料" tabindex="-1">参考资料 <a class="header-anchor" href="#参考资料" aria-label="Permalink to &quot;参考资料&quot;">​</a></h2><ul><li>GNU C Library (glibc)</li><li><a href="https://docs.oracle.com/javase/8/docs/technotes/guides/troubleshoot/tooldescr007.html" target="_blank" rel="noreferrer">Native Memory Tracking在新窗口打开</a></li><li><a href="https://github.com/gperftools/gperftools" target="_blank" rel="noreferrer">gperftools在新窗口打开</a></li><li><a href="https://github.com/btraceio/btrace" target="_blank" rel="noreferrer">Btrace在新窗口打开</a></li></ul><p>作者简介</p><ul><li>纪兵，2015年加入美团，目前主要从事酒店C端相关的工作。</li></ul><p>本文转自 <a href="https://pdai.tech" target="_blank" rel="noreferrer">https://pdai.tech</a>，如有侵权，请联系删除。</p>',66)]))}const P=s(_,[["render",j]]);export{A as __pageData,P as default};
